% This file was created with Citavi 6.3.0.0

@incollection{AlmondZapataRivera.2019,
 author = {Almond, Russell G. and Zapata-Rivera, Juan-Diego},
 title = {Bayesian Networks},
 pages = {81--106},
 volume = {51},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-030-05583-7},
 editor = {{von Davier}, Matthias and Lee, Young-Sun},
 booktitle = {Handbook of Diagnostic Classification Models},
 year = {2019},
 address = {Cham},
 doi = {10.1007/978-3-030-05584-4{\textunderscore }4}
}


@article{Aryadoust.2018,
 author = {Aryadoust, Vahid},
 year = {2018},
 title = {A Cognitive Diagnostic Assessment Study of the Listening Test of the Singapore--Cambridge General Certificate of Education O-Level: Application of {DINA}, {DINO}, {G-DINA}, {HO-DINA}, and {RRUM}},
 pages = {1--24},
 volume = {63},
 number = {2},
 issn = {1090-4018},
 journal = {International Journal of Listening},
 doi = {10.1080/10904018.2018.1500915},
 file = {Aryadoust-2018-CDA.pdf}
}


@article{Bley.2017,
 abstract = {Empirical Research in Vocational Education and Training, doi:10.1186/s40461-017-0049-0},
 author = {Bley, Sandra},
 year = {2017},
 title = {Developing and validating a technology-based diagnostic assessment using the evidence-centered game design approach: an example of intrapreneurship competence},
 keywords = {Cognitive diagnostic assessment;Diagnostic assessment;Evidence-centered design;Principled assessment design;Technology-based assessment;Vocational education and training intrapreneurship},
 pages = {281},
 volume = {9},
 number = {1},
 journal = {Empirical Research in Vocational Education and Training},
 doi = {10.1186/s40461-017-0049-0},
 file = {document(6).pdf}
}


@article{Bolt.2007,
 author = {Bolt, Daniel},
 year = {2007},
 title = {The Present and Future of {IRT}-Based Cognitive Diagnostic Models ({ICDM}) and Related Methods},
 pages = {377--383},
 volume = {44},
 number = {4},
 issn = {0022-0655},
 journal = {Journal of Educational Measurement},
 doi = {10.1111/j.1745-3984.2007.00045.x},
 file = {Bolt - 2007 - The Present and Future of IRT-Based Cognitive Diagnostic Models (ICDMs) and Related Methods.pdf}
}


@article{BradshawIzsak.2014,
 author = {Bradshaw, Laine and Izs{\'a}k, Andrew and Templin, Jonathan and Jacobson, Erik},
 year = {2014},
 title = {Diagnosing Teachers' Understandings of Rational Numbers: Building a Multidimensional Test Within the Diagnostic Classification Framework},
 pages = {2--14},
 volume = {33},
 number = {1},
 issn = {07311745},
 journal = {Educational Measurement: Issues and Practice},
 doi = {10.1111/emip.12020},
 file = {Bradshaw_et_al-2014-Educational_Measurement%3A_Issues_and_Practice.pdf}
}


@article{BradshawTemplin.2014,
 abstract = {Traditional testing procedures typically utilize unidimensional item response theory (IRT) models to provide a single, continuous estimate of a student's overall ability. Advances in psychometrics have focused on measuring multiple dimensions of ability to provide more detailed feedback for students, teachers, and other stakeholders. Diagnostic classification models (DCMs) provide multidimensional feedback by using categorical latent variables that represent distinct skills underlying a test that students may or may not have mastered. The Scaling Individuals and Classifying Misconceptions (SICM) model is presented as a combination of a unidimensional IRT model and a DCM where the categorical latent variables represent misconceptions instead of skills. In addition to an estimate of ability along a latent continuum, the SICM model provides multidimensional, diagnostic feedback in the form of statistical estimates of probabilities that students have certain misconceptions. Through an empirical data analysis, we show how this additional feedback can be used by stakeholders to tailor instruction for students' needs. We also provide results from a simulation study that demonstrate that the SICM MCMC estimation algorithm yields reasonably accurate estimates under large-scale testing conditions.},
 author = {Bradshaw, Laine and Templin, Jonathan},
 year = {2014},
 title = {Combining item response theory and diagnostic classification models: a psychometric model for scaling ability and diagnosing misconceptions},
 keywords = {Classification/methods;Educational Measurement/standards;Humans;Psychological Theory;Psychometrics/methods;Statistics as Topic/methods},
 pages = {403--425},
 volume = {79},
 number = {3},
 journal = {Psychometrika},
 doi = {10.1007/s11336-013-9350-4},
 file = {Bradshaw, Templin - 2014 - Combining Item Response Theory and Diagnostic Classification Models A Psychometric Model for Scaling Ability.pdf},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/25205005}
}


@article{BrianW.JunkerandKlaasSijtsma.2001,
 abstract = {Applied Psychological Measurement 2001.25:258-272},
 author = {Junker, Brian W. and Sijtsma, Klaas},
 year = {2001},
 title = {Cognitive Assessment Models with Few Assumptions, and Connections with Nonparametric Item Response Theory},
 urldate = {2/7/2020},
 pages = {258--272},
 issn = {0146-6216},
 journal = {Applied Psychological Measurement},
 file = {Brian W. Junker and Klaas Sijtsma - Cognitive Assessment Models with Few (2).pdf}
}


@article{BriggsCirci.2017,
 author = {Briggs, Derek C. and Circi, Ruhan},
 year = {2017},
 title = {Challenges to the Use of Artificial Neural Networks for Diagnostic Classifications with Student Test Data},
 pages = {302--321},
 volume = {17},
 number = {4},
 issn = {1530-5058},
 journal = {International Journal of Testing},
 doi = {10.1080/15305058.2017.1297816},
 file = {Briggs, Circi - 2017 - Challenges to the Use of Artificial Neural Networks for Diagnostic Classifications with Student Test Data.pdf}
}


@article{CarragherTemplin.2019,
 abstract = {Educational Measurement: Issues and Practice 2019.38:89-91},
 author = {Carragher, Natacha and Templin, Jonathan and Jones, Phillip and Shulruf, Boaz and Velan, Gary},
 year = {2019},
 title = {Digital Module 04: Diagnostic Measurement: Modeling Checklists for Practitioners https://ncme.elevate.commpartners.com},
 urldate = {2/6/2020},
 pages = {89--91},
 volume = {38},
 number = {1},
 issn = {07311745},
 journal = {Educational Measurement: Issues and Practice},
 doi = {10.1111/emip.12251},
 file = {Digital Module 04.pdf}
}


@article{Chen.2017,
 abstract = {Q-matrix validation is of increasing concern due to the significance and subjective tendency of Q-matrix construction in the modeling process. This research proposes a residual-based approach to empirically validate Q-matrix specification based on a combination of fit measures. The approach separates Q-matrix validation into four logical steps, including the test-level evaluation, possible distinction between attribute-level and item-level misspecifications, identification of the hit item, and fit information to aid in item adjustment. Through simulation studies and real-life examples, it is shown that the misspecified items can be detected as the hit item and adjusted sequentially when the misspecification occurs at the item level or at random. Adjustment can be based on the maximum reduction of the test-level measures. When adjustment of individual items tends to be useless, attribute-level misspecification is of concern. The approach can accommodate a variety of cognitive diagnosis models (CDMs) and be extended to cover other response formats.},
 author = {Chen, Jinsong},
 year = {2017},
 title = {A Residual-Based Approach to Validate Q-Matrix Specifications},
 pages = {277--293},
 volume = {41},
 number = {4},
 issn = {0146-6216},
 journal = {Applied Psychological Measurement},
 doi = {10.1177/0146621616686021},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/29881093},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5978584},
 file = {Chen - 2017 - A Residual-Based Approach to Validate Q-Matrix Specifications.pdf}
}


@article{Chen.2018,
 abstract = {Latent class models are powerful tools in psychological and educational measurement. These models classify individuals into subgroups based on a set of manifest variables, assisting decision making in a diagnostic system. In this article, based on information theory, the authors propose a mutual information reliability (MIR) coefficient that summaries the measurement quality of latent class models, where the latent variables being measured are categorical. The proposed coefficient is analogous to a version of reliability coefficient for item response theory models and meets the general concept of measurement reliability in the Standards for Educational and Psychological Testing. The proposed coefficient can also be viewed as an extension of the McFadden's pseudo R-square coefficient, which evaluates the goodness-of-fit of logistic regression model, to latent class models. Thanks to several information-theoretic inequalities, the MIR coefficient is unitless, lies between 0 and 1, and receives good interpretation from a measurement point of view. The coefficient can be applied to both fixed and computerized adaptive testing designs. The performance of the MIR coefficient is demonstrated by simulated examples.},
 author = {Chen, Yunxiao and Liu, Yang and Xu, Shuangshuang},
 year = {2018},
 title = {Mutual Information Reliability for Latent Class Analysis},
 keywords = {Cognitive diagnosis model;computerized adaptive testing;item response theory;Latent Class Analysis;mastery test;mutual information;pseudo R-square;reliability;Shannon entropy},
 pages = {460--477},
 volume = {42},
 number = {6},
 issn = {0146-6216},
 journal = {Applied Psychological Measurement},
 doi = {10.1177/0146621617748324},
 file = {0146621617748324.pdf},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6373856},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/30787488}
}


@article{ChenCulpepper.2018,
 abstract = {The increasing presence of electronic and online learning resources presents challenges and opportunities for psychometric techniques that can assist in the measurement of abilities and even hasten their mastery. Cognitive diagnosis models (CDMs) are ideal for tracking many fine-grained skills that comprise a domain, and can assist in carefully navigating through the training and assessment of these skills in e-learning applications. A class of CDMs for modeling changes in attributes is proposed, which is referred to as learning trajectories. The authors focus on the development of Bayesian procedures for estimating parameters of a first-order hidden Markov model. An application of the developed model to a spatial rotation experimental intervention is presented.},
 author = {Chen, Yinghan and Culpepper, Steven Andrew and Wang, Shiyu and Douglas, Jeffrey},
 year = {2018},
 title = {A Hidden Markov Model for Learning Trajectories in Cognitive Diagnosis With Application to Spatial Rotation Skills},
 pages = {5--23},
 volume = {42},
 number = {1},
 issn = {0146-6216},
 journal = {Applied Psychological Measurement},
 doi = {10.1177/0146621617721250},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5978590},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/29881110},
 file = {Chen, Culpepper et al. 2018 - A Hidden Markov Model.pdf}
}


@article{ChenCulpepperLiang.2020,
 abstract = {Cognitive diagnostic models (CDMs) are latent variable models developed to infer latent skills, knowledge, or personalities that underlie responses to educational, psychological, and social science tests and measures. Recent research focused on theory and methods for using sparse latent class models (SLCMs) in an exploratory fashion to infer the latent processes and structure underlying responses. We report new theoretical results about sufficient conditions for generic identifiability of SLCM parameters. An important contribution for practice is that our new generic identifiability conditions are more likely to be satisfied in empirical applications than existing conditions that ensure strict identifiability. Learning the underlying latent structure can be formulated as a variable selection problem. We develop a new Bayesian variable selection algorithm that explicitly enforces generic identifiability conditions and monotonicity of item response functions to ensure valid posterior inference. We present Monte Carlo simulation results to support accurate inferences and discuss the implications of our findings for future SLCM research and educational testing.},
 author = {Chen, Yinyin and Culpepper, Steven and Liang, Feng},
 year = {2020},
 title = {A Sparse Latent Class Model for Cognitive Diagnosis},
 keywords = {Bayesian variable selection;Identifiability;sparse latent class models;sparse latent class models,Bayesian variable selection,identifiability},
 urldate = {2/19/2020},
 journal = {Psychometrika},
 doi = {10.1007/s11336-019-09693-2},
 file = {de8d8e22-29c5-46a6-8b80-196e912299cd:C\:\\Users\\wench\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\hzsl0hndqon2ipk638qbntgdllf1r1q5dxzhi26xkkmxy8hcs6\\Citavi Attachments\\de8d8e22-29c5-46a6-8b80-196e912299cd.pdf:pdf}
}


@article{ChendelaTorre.2013,
 author = {Chen, Jinsong and {de la Torre}, Jimmy},
 year = {2013},
 title = {A General Cognitive Diagnosis Model for Expert-Defined Polytomous Attributes},
 pages = {419--437},
 volume = {37},
 number = {6},
 issn = {0146-6216},
 journal = {Applied Psychological Measurement},
 doi = {10.1177/0146621613479818},
 file = {Chen, de la Torre - 2013 - A General Cognitive Diagnosis Model for Expert-Defined Polytomous Attributes.pdf}
}


@article{ChendelaTorre.2018,
 abstract = {Although considerable developments have been added to the cognitive diagnosis modeling literature recently, most have been conducted for dichotomous responses only. This research proposes a general cognitive diagnosis model for polytomous responses-the general polytomous diagnosis model (GPDM), which combines the G-DINA modeling process for dichotomous responses with the item-splitting process for polytomous responses. The polytomous items are specified similar to dichotomous items in the Q-matrix, and the MML estimation is implemented using an EM algorithm. Under the general framework, different saturated forms, and some reduced forms, can be transformed linearly. Model assessment and adjustment under the dichotomous context can be extended to polytomous responses. This simulation study demonstrates the effectiveness of the model when comparing the two response types. The real-data example further illustrates how the proposed model can make a difference in practice.},
 author = {Chen, Jinsong and {de la Torre}, Jimmy},
 year = {2018},
 title = {Introducing the General Polytomous Diagnosis Modeling Framework},
 keywords = {CDM;item-splitting;MML;polytomous responses;saturated model},
 pages = {1474},
 volume = {9},
 issn = {1664-1078},
 journal = {Frontiers in psychology},
 doi = {10.3389/fpsyg.2018.01474},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6113892},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/30186195},
 file = {fpsyg-09-01474.pdf}
}


@article{ChendelaTorreZhang.2013,
 author = {Chen, Jinsong and {de la Torre}, Jimmy and Zhang, Zao},
 year = {2013},
 title = {Relative and Absolute Fit Evaluation in Cognitive Diagnosis Modeling},
 pages = {123--140},
 volume = {50},
 number = {2},
 issn = {0022-0655},
 journal = {Journal of Educational Measurement},
 doi = {10.1111/j.1745-3984.2012.00185.x},
 file = {Chen, de la Torre, Zhang - 2013 - Relative and Absolute Fit Evaluation in Cognitive Diagnosis Modeling.pdf}
}


@article{Cheng.2009,
 author = {Cheng, Ying},
 year = {2009},
 title = {When Cognitive Diagnosis Meets Computerized Adaptive Testing: {CD-CAT}},
 urldate = {2/23/2020},
 pages = {619--632},
 volume = {74},
 number = {4},
 journal = {Psychometrika},
 doi = {10.1007/s11336-009-9123-2},
 file = {a083bb10-d207-4a00-8427-8ac61d1e8b30:C\:\\Users\\wench\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\hzsl0hndqon2ipk638qbntgdllf1r1q5dxzhi26xkkmxy8hcs6\\Citavi Attachments\\a083bb10-d207-4a00-8427-8ac61d1e8b30.pdf:pdf}
}


@article{ChenLi.2016,
 abstract = {Diagnostic classification models are confirmatory in the sense that the relationship between the latent attributes and responses to items is specified or parameterized. Such models are readily interpretable with each component of the model usually having a practical meaning. However, parameterized diagnostic classification models are sometimes too simple to capture all the data patterns, resulting in significant model lack of fit. In this paper, we attempt to obtain a compromise between interpretability and goodness of fit by regularizing a latent class model. Our approach starts with minimal assumptions on the data structure, followed by suitable regularization to reduce complexity, so that readily interpretable, yet flexible model is obtained. An expectation-maximization-type algorithm is developed for efficient computation. It is shown that the proposed approach enjoys good theoretical properties. Results from simulation studies and a real application are presented.},
 author = {Chen, Yunxiao and Li, Xiaoou and Liu, Jingchen and Ying, Zhiliang},
 year = {2016},
 title = {Regularized Latent Class Analysis with Application in Cognitive Diagnosis},
 keywords = {consistency;Diagnostic classification models;EM algorithm;Latent Class Analysis;regularization},
 journal = {Psychometrika},
 doi = {10.1007/s11336-016-9545-6},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/27905058},
 file = {4229bcc5-3ba0-4c21-867e-dc0853b7b1d4:C\:\\Users\\wench\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\hzsl0hndqon2ipk638qbntgdllf1r1q5dxzhi26xkkmxy8hcs6\\Citavi Attachments\\4229bcc5-3ba0-4c21-867e-dc0853b7b1d4.pdf:pdf}
}


@article{ChenLiu.2018,
 abstract = {The performance of the limited-information statistic M2 for diagnostic classification models (DCMs) is under-investigated in the current literature. Specifically, the investigations of M2 for specific DCMs rather than general modeling frameworks are needed. This article aims to demonstrate the usefulness of M2 in hierarchical diagnostic classification models (HDCMs). The performance of M2 in evaluating the fit of HDCMs was investigated in the presence of four types of attribute hierarchies. Two simulation studies were conducted to examine Type I error rates and statistical power of M2 under different simulation conditions, respectively. The findings suggest acceptable Type I error rates control of M2 as well as high statistical power under the conditions of a Q-matrix misspecification and the DINA model misspecification. The data of Examination for the Certificate of Proficiency in English (ECPE) were used to empirically illustrate the suitability of M2 in practice.},
 author = {Chen, Fu and Liu, Yanlou and Xin, Tao and Cui, Ying},
 year = {2018},
 title = {Applying the $M_2$ Statistic to Evaluate the Fit of Diagnostic Classification Models in the Presence of Attribute Hierarchies},
 keywords = {absolute fit test;attribute hierarchies;Diagnostic classification models;goodness-of-fit;limited-information test statistics},
 pages = {1875},
 volume = {9},
 issn = {1664-1078},
 journal = {Frontiers in psychology},
 doi = {10.3389/fpsyg.2018.01875},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/30356781},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6189476},
 file = {fpsyg-09-01875.pdf}
}


@article{ChenXin.2012,
 author = {Chen, Ping and Xin, Tao and Wang, Chun and Chang, Hua-Hua},
 year = {2012},
 title = {Online Calibration Methods for the {DINA} Model with Independent Attributes in {CD-CAT}},
 urldate = {2/23/2020},
 pages = {201--222},
 volume = {77},
 number = {2},
 journal = {Psychometrika},
 doi = {10.1007/s11336-012-9255-7},
 file = {54bb4c33-475a-4184-84a8-46380da1c520:C\:\\Users\\wench\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\hzsl0hndqon2ipk638qbntgdllf1r1q5dxzhi26xkkmxy8hcs6\\Citavi Attachments\\54bb4c33-475a-4184-84a8-46380da1c520.pdf:pdf}
}


@article{ChenZhou.2017,
 abstract = {Most psychological questionnaires face issues of response bias in respondent-reported scales, inadequacy for criterion-reference testing, or difficulty in estimating a large number of latent traits. Situational tests together with the general nominal diagnosis model framework provide a viable alternative to alleviate these concerns. Under this framework, there are different ways to design situationally nominal items, which can offer more flexibility for test development. Any response bias remaining with respondent-reported questionnaires may be addressed with appropriate test designs. The saturated model subsumes different reduced forms that can help inform whether the test is designed as expected. Two simulation studies are presented to demonstrate the effectiveness of the models and designs.},
 author = {Chen, Jinsong and Zhou, Hui},
 year = {2017},
 title = {Test designs and modeling under the general nominal diagnosis model framework},
 keywords = {Child;Computer Simulation;Humans;Models, Theoretical;Personality Tests;Research Design;Surveys and Questionnaires},
 pages = {e0180016},
 volume = {12},
 number = {6},
 journal = {PloS one},
 doi = {10.1371/journal.pone.0180016},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5482485},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/28644865},
 file = {Chen, Zhou - 2017 - Test designs and modeling under the general nominal diagnosis model framework.pdf}
}


@article{Chiu.2013,
 author = {Chiu, Chia-Yi},
 year = {2013},
 title = {Statistical Refinement of the {Q}-Matrix in Cognitive Diagnosis},
 pages = {598--618},
 volume = {37},
 number = {8},
 issn = {0146-6216},
 journal = {Applied Psychological Measurement},
 doi = {10.1177/0146621613488436},
 file = {Chiu - 2013 - Statistical Refinement of the Q-Matrix in Cognitive Diagnosis.pdf}
}


@article{ChiuDouglasLi.2009,
 author = {Chiu, Chia-Yi and Douglas, Jeffrey A. and Li, Xiaodong},
 year = {2009},
 title = {Cluster Analysis for Cognitive Diagnosis: Theory and Applications},
 pages = {633--665},
 volume = {74},
 number = {4},
 journal = {Psychometrika},
 doi = {10.1007/s11336-009-9125-0},
 file = {Chiu, Douglas, Li - 2009 - Cluster analysis for cognitive diagnosis Theory and applications.pdf}
}


@incollection{ChiuKohn.2019,
 author = {Chiu, Chia-Yi and K{\"o}hn, Hans-Friedrich},
 title = {Nonparametric Methods in Cognitively Diagnostic Assessment},
 pages = {107--132},
 volume = {46},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-030-05583-7},
 editor = {{von Davier}, Matthias and Lee, Young-Sun},
 booktitle = {Handbook of Diagnostic Classification Models},
 year = {2019},
 address = {Cham},
 doi = {10.1007/978-3-030-05584-4{\textunderscore }5}
}


@article{ChiuSunBian.2018,
 abstract = {The focus of cognitive diagnosis (CD) is on evaluating an examinee's strengths and weaknesses in terms of cognitive skills learned and skills that need study. Current methods for fitting CD models (CDMs) work well for large-scale assessments, where the data of hundreds or thousands of examinees are available. However, the development of CD-based assessment tools that can be used in small-scale test settings, say, for monitoring the instruction and learning process at the classroom level has not kept up with the rapid pace at which research and development proceeded for large-scale assessments. The main reason is that the sample sizes of the small-scale test settings are simply too small to guarantee the reliable estimation of item parameters and examinees' proficiency class membership. In this article, a general nonparametric classification (GNPC) method that allows for assigning examinees to the correct proficiency classes with a high rate when sample sizes are at the classroom level is proposed as an extension of the nonparametric classification (NPC) method (Chiu and Douglas in J Classif 30:225-250, 2013). The proposed method remedies the shortcomings of the NPC method and can accommodate any CDM. The theoretical justification and the empirical studies are presented based on the saturated general CDMs, supporting the legitimacy of using the GNPC method with any CDM. The results from the simulation studies and real data analysis show that the GNPC method outperforms the general CDMs when samples are small.},
 author = {Chiu, Chia-Yi and Sun, Yan and Bian, Yanhong},
 year = {2018},
 title = {Cognitive Diagnosis for Small Educational Programs: The General Nonparametric Classification Method},
 keywords = {Cognition;Computer Simulation;Educational Measurement/methods;Humans;Learning;Mathematical Concepts;Multivariate analysis;Psychological Tests;Psychometrics/methods;Software;Statistics, Nonparametric},
 pages = {355--375},
 volume = {83},
 number = {2},
 journal = {Psychometrika},
 doi = {10.1007/s11336-017-9595-4},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/29150816}
}


@article{Culpepper.2019b,
 abstract = {Diagnostic models (DMs) provide researchers and practitioners with tools to classify respondents into substantively relevant classes. DMs are widely applied to binary response data; however, binary response models are not applicable to the wealth of ordinal data collected by educational, psychological, and behavioral researchers. Prior research developed confirmatory ordinal DMs that require expert knowledge to specify the underlying structure. This paper introduces an exploratory DM for ordinal data. In particular, we present an exploratory ordinal DM, which uses a cumulative probit link along with Bayesian variable selection techniques to uncover the latent structure. Furthermore, we discuss new identifiability conditions for structured multinomial mixture models with binary attributes. We provide evidence of accurate parameter recovery in a Monte Carlo simulation study across moderate to large sample sizes. We apply the model to twelve items from the public-use, Early Childhood Longitudinal Study, Kindergarten Class of 1998-1999 approaches to learning and self-description questionnaire and report evidence to support a three-attribute solution with eight classes to describe the latent structure underlying the teacher and parent ratings. In short, the developed methodology contributes to the development of ordinal DMs and broadens their applicability to address theoretical and substantive issues more generally across the social sciences.},
 author = {Culpepper, Steven Andrew},
 year = {2019},
 title = {An Exploratory Diagnostic Model for Ordinal Responses with Binary Attributes: Identifiability and Estimation},
 keywords = {Bayesian;cognitive diagnosis;latent class;multivariate ordinal data;multivariate ordinal data,cognitive diagnosis,latent class,Bayesian},
 urldate = {2/7/2020},
 pages = {921--940},
 volume = {84},
 number = {4},
 journal = {Psychometrika},
 doi = {10.1007/s11336-019-09683-4},
 file = {Culpepper 2019 - An Exploratory Diagnostic Model.pdf},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/31432312}
}


@article{CulpepperChen.2019,
 author = {Culpepper, Steven Andrew and Chen, Yinghan},
 year = {2019},
 title = {Development and Application of an Exploratory Reduced Reparameterized Unified Model},
 pages = {3--24},
 volume = {44},
 number = {1},
 issn = {1076-9986},
 journal = {Journal of Educational and Behavioral Statistics},
 doi = {10.3102/1076998618791306}
}


@article{daSilvaOliveira.2018,
 abstract = {The deterministic inputs, noisy, {\textquotedbl}and{\textquotedbl} gate (DINA) model is a popular cognitive diagnosis model (CDM) in psychology and psychometrics used to identify test takers' profiles with respect to a set of latent attributes or skills. In this work, we propose an estimation method for the DINA model with the No-U-Turn Sampler (NUTS) algorithm, an extension to Hamiltonian Monte Carlo (HMC) method. We conduct a simulation study in order to evaluate the parameter recovery and efficiency of this new Markov chain Monte Carlo method and to compare it with two other Bayesian methods, the Metropolis Hastings and Gibbs sampling algorithms, and with a frequentist method, using the Expectation-Maximization (EM) algorithm. The results indicated that NUTS algorithm employed in the DINA model properly recovers all parameters and is accurate for all simulated scenarios. We apply this methodology in the mental health area in order to develop a new method of classification for respondents to the Beck Depression Inventory. The implementation of this method for the DINA model applied to other psychological tests has the potential to improve the medical diagnostic process.},
 author = {{da Silva}, Marcelo A. and de Oliveira, Eduardo S. B. and von Davier, Alina A. and Baz{\'a}n, Jorge L.},
 year = {2018},
 title = {Estimating the {DINA} model parameters using the No-U-Turn Sampler},
 keywords = {Algorithms;Biometry/methods;Cognition;Depression/physiopathology/psychology;Humans;Models, Statistical;Monte Carlo Method;Psychometrics},
 pages = {352--368},
 volume = {60},
 number = {2},
 journal = {Biometrical journal. Biometrische Zeitschrift},
 doi = {10.1002/bimj.201600225},
 file = {bimj.201600225.pdf},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/29194715}
}


@article{DeCarlo.2012,
 author = {DeCarlo, Lawrence T.},
 year = {2012},
 title = {Recognizing Uncertainty in the {Q}-Matrix via a Bayesian Extension of the {DINA} Model},
 pages = {447--468},
 volume = {36},
 number = {6},
 issn = {0146-6216},
 journal = {Applied Psychological Measurement},
 doi = {10.1177/0146621612449069},
 file = {DeCarlo - 2012 - Recognizing Uncertainty in the Q-Matrix via a Bayesian Extension of the DINA Model.pdf}
}


@article{delaTorre.2008,
 author = {{de la Torre}, Jimmy},
 year = {2008},
 title = {An Empirically Based Method of {Q}-Matrix Validation for the {DINA} Model: Development and Applications},
 pages = {343--362},
 volume = {45},
 number = {4},
 issn = {0022-0655},
 journal = {Journal of Educational Measurement},
 doi = {10.1111/j.1745-3984.2008.00069.x},
 file = {de la Torre - 2008 - An Empirically Based Method of Q-Matrix Validation for the DINA Model Development and Applications.pdf}
}


@article{delaTorre.2009b,
 author = {{de la Torre}, Jimmy},
 year = {2009},
 title = {{DINA} Model and Parameter Estimation: A Didactic},
 pages = {115--130},
 volume = {34},
 number = {1},
 issn = {1076-9986},
 journal = {Journal of Educational and Behavioral Statistics},
 doi = {10.3102/1076998607309474},
 file = {de la Torre - 2008 - DINA Model and Parameter Estimation A Didactic.pdf}
}


@article{delaTorre.2009d,
 author = {{de la Torre}, Jimmy},
 year = {2009},
 title = {A Cognitive Diagnosis Model for Cognitively Based Multiple-Choice Options},
 pages = {163--183},
 volume = {33},
 number = {3},
 issn = {0146-6216},
 journal = {Applied Psychological Measurement},
 doi = {10.1177/0146621608320523},
 file = {de la Torre - 2009 - A Cognitive Diagnosis Model for Cognitively Based Multiple-Choice Options.pdf}
}


@article{delaTorre.2011,
 author = {{de la Torre}, Jimmy},
 year = {2011},
 title = {The Generalized {DINA} Model Framework},
 pages = {179--199},
 volume = {76},
 number = {2},
 journal = {Psychometrika},
 doi = {10.1007/s11336-011-9207-7},
 file = {Torre - 2011 - The generalized DINA model framework.pdf}
}


@article{delaTorreChiu.2016,
 abstract = {In contrast to unidimensional item response models that postulate a single underlying proficiency, cognitive diagnosis models (CDMs) posit multiple, discrete skills or attributes, thus allowing CDMs to provide a finer-grained assessment of examinees' test performance. A common component of CDMs for specifying the attributes required for each item is the Q-matrix. Although construction of Q-matrix is typically performed by domain experts, it nonetheless, to a large extent, remains a subjective process, and misspecifications in the Q-matrix, if left unchecked, can have important practical implications. To address this concern, this paper proposes a discrimination index that can be used with a wide class of CDM subsumed by the generalized deterministic input, noisy {\textquotedbl}and{\textquotedbl} gate model to empirically validate the Q-matrix specifications by identifying and replacing misspecified entries in the Q-matrix. The rationale for using the index as the basis for a proposed validation method is provided in the form of mathematical proofs to several relevant lemmas and a theorem. The feasibility of the proposed method was examined using simulated data generated under various conditions. The proposed method is illustrated using fraction subtraction data.},
 author = {{de la Torre}, Jimmy and Chiu, Chia-Yi},
 year = {2016},
 title = {A General Method of Empirical {Q}-matrix Validation},
 keywords = {Adolescent;Algorithms;Child;Cognition;cognitive diagnosis G-DINA Q-matrix validation MMLE;Educational Measurement;Feasibility Studies;Humans;Models, Psychological;Models, Statistical;Psychometrics;Reproducibility of Results;Statistics as Topic},
 pages = {253--273},
 volume = {81},
 number = {2},
 journal = {Psychometrika},
 doi = {10.1007/s11336-015-9467-8},
 file = {de la Torre, Chiu - 2016 - A General Method of Empirical Q-matrix Validation.pdf},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/25943366}
}


@article{delaTorreDouglas.2004,
 author = {{de la Torre}, Jimmy and Douglas, Jeffrey A.},
 year = {2004},
 title = {Higher-order latent trait models for cognitive diagnosis},
 pages = {333--353},
 volume = {69},
 number = {3},
 journal = {Psychometrika},
 doi = {10.1007/bf02295640},
 file = {de la Torre, Douglas - 2004 - Higher-order latent trait models for cognitive diagnosis.pdf}
}


@article{delaTorreDouglas.2008,
 author = {{de la Torre}, Jimmy and Douglas, Jeffrey A.},
 year = {2008},
 title = {Model Evaluation and Multiple Strategies in Cognitive Diagnosis: An Analysis of Fraction Subtraction Data},
 pages = {595--624},
 volume = {73},
 number = {4},
 journal = {Psychometrika},
 doi = {10.1007/s11336-008-9063-2},
 file = {de la Torre, Douglas - 2008 - Model evaluation and multiple strategies in cognitive diagnosis An analysis of fraction subtraction data.pdf}
}


@article{delaTorreLee.2013,
 author = {{de la Torre}, Jimmy and Lee, Young-Sun},
 year = {2013},
 title = {Evaluating the Wald Test for Item-Level Comparison of Saturated and Reduced Models in Cognitive Diagnosis},
 pages = {355--373},
 volume = {50},
 number = {4},
 issn = {0022-0655},
 journal = {Journal of Educational Measurement},
 doi = {10.1111/jedm.12022},
 file = {de la Torre, Lee - 2013 - Evaluating the Wald Test for Item-Level Comparison of Saturated and Reduced Models in Cognitive Diagnosis.pdf}
}


@article{delaTorreMinchen.2014,
 abstract = {Psicolog{\'i}a Educativa, 20 (2014) 89-97. doi:10.1016/j.pse.2014.11.001},
 author = {{de la Torre}, Jimmy and Minchen, Nathan},
 year = {2014},
 title = {Cognitively Diagnostic Assessments and the Cognitive Diagnosis Model Framework},
 keywords = {Assessment triangle;Cognitive diagnosis model;Cognitively diagnostic assessments;Dise{\~n}o centrado en la evidencia;Evaluaci{\'o}n para el diagn{\'o}stico cognitivo;Evidence-centered design;Modelo de diagn{\'o}stico cognitivo;Tri{\'a}ngulo de evaluaci{\'o}n},
 pages = {89--97},
 volume = {20},
 number = {2},
 issn = {1135755X},
 journal = {Psicolog{\'i}a Educativa},
 doi = {10.1016/j.pse.2014.11.001},
 file = {de la Torre, Minchen - 2014 - Cognitively Diagnostic Assessments and the Cognitive Diagnosis Model Framework.pdf}
}


@article{delaTorrevanderArkRossi.2018,
 author = {{de la Torre}, Jimmy and {van der Ark}, L. Andries and Rossi, Gina},
 year = {2018},
 title = {Analysis of Clinical Data From a Cognitive Diagnosis Modeling Framework},
 pages = {281--296},
 volume = {51},
 number = {4},
 issn = {0748-1756},
 journal = {Measurement and Evaluation in Counseling and Development},
 doi = {10.1080/07481756.2017.1327286},
 file = {de la Torre, van der Ark, Rossi - 2015 - Analysis of clinical data from cognitive diagnosis modeling framework-annotated.pdf}
}


@article{DiBelloHensonStout.2015b,
 abstract = {This article proposes a new family of diagnostic classification models (DCM) called the Generalized Diagnostic Classification Models for Multiple Choice Option-Based Scoring (GDCM-MC). The GDCM-MC is created for multiple choice assessments with response options designed to attract particular kinds of student thinking and understanding, both desired (correct) thinking and problematic (incorrect or partially correct) thinking. Key features that combine to distinguish GDCM-MC are: (a) an expanded latent space that can include both desirable and problematic facets of thinking, (b) an expanded Q matrix that includes a row for each response option and that uses a three-valued coding scheme to specify which latent states are strongly attracted to that option, (c) a guessing component that responds to the forced choice aspect of multiple choice questions, and (d) a general modeling framework that can incorporate the diagnostic modeling functionality of almost any dichotomous DCM, such as deterministic input, noisy ``and'' gate (DINA), reparameterized unified model (RUM), loglinear cognitive diagnosis model (LCDM), or general diagnostic model (GDM). The article discusses these four components and presents the GDCM-MC model equation as a mixture of cognitive and guessing components. Two identifiability theorems are presented. A Bayesian Markov Chain Monte Carlo (MCMC) model estimation algorithm is discussed, and real and simulated data studies are reported.},
 author = {DiBello, Louis V. and Henson, Robert A. and Stout, William F.},
 year = {2015},
 title = {A Family of Generalized Diagnostic Classification Models for Multiple Choice Option-Based Scoring},
 pages = {62--79},
 volume = {39},
 number = {1},
 issn = {0146-6216},
 journal = {Applied Psychological Measurement},
 doi = {10.1177/0146621614561315},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/29880994},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5978573}
}


@article{DiBelloStout.2007,
 author = {DiBello, Louis V. and Stout, William},
 year = {2007},
 title = {Guest Editors' Introduction and Overview: {IRT}-Based Cognitive Diagnostic Models and Related Methods},
 pages = {285--291},
 volume = {44},
 number = {4},
 issn = {0022-0655},
 journal = {Journal of Educational Measurement},
 doi = {10.1111/j.1745-3984.2007.00039.x},
 file = {DiBello, Stout - 2007 - Guest Editors' Introduction and Overview IRT-Based Cognitive Diagnostic Models and Related Methods.pdf}
}


@incollection{DiBelloStoutRoussos.1995,
 author = {DiBello, Louis V. and Stout, William F. and Roussos, Louis},
 title = {Unified cognitive/psychometric diagnostic assessment likelihood-based classification techniques},
 pages = {361--390},
 publisher = {{Hills-dale, NJ:Erlbaum}},
 editor = {Nichols, Paul D. and Chipman, S. F. and Brennan, Robert L.},
 booktitle = {Cognitively Diagnostic Assessment},
 year = {1995}
}


@article{FalmagneKoppen.1990,
 author = {Falmagne, Jean-Claude and Koppen, Mathieu and Villano, Michael and Doignon, Jean-Paul and al, et},
 year = {1990},
 title = {Introduction to knowledge spaces: How to build, test, and search them},
 pages = {201--224},
 volume = {97},
 number = {2},
 journal = {Psychological review},
 doi = {10.1037//0033-295x.97.2.201},
 file = {Falmagne et al. - 1990 - Introduction to Knowledge Spaces How to Build , Test , and Search Therff.pdf}
}


@article{FangLiuYing.2019,
 abstract = {This paper establishes fundamental results for statistical analysis based on diagnostic classification models (DCMs). The results are developed at a high level of generality and are applicable to essentially all diagnostic classification models. In particular, we establish identifiability results for various modeling parameters, notably item response probabilities, attribute distribution, and Q-matrix-induced partial information structure. These results are stated under a general setting of latent class models. Through a nonparametric Bayes approach, we construct an estimator that can be shown to be consistent when the identifiability conditions are satisfied. Simulation results show that these estimators perform well under various model settings. We also apply the proposed method to a dataset from the National Epidemiological Survey on Alcohol and Related Conditions (NESARC).},
 author = {Fang, Guanhua and Liu, Jingchen and Ying, Zhiliang},
 year = {2019},
 title = {On the Identifiability of Diagnostic Classification Models},
 keywords = {Adult;Bayes Theorem;Cognition;Computer Simulation;Diagnosis, Computer-Assisted/methods;Diagnostic classification models;Dirichlet allocation;Humans;Identifiability;Latent Class Analysis;Male;Middle Aged;Models, Statistical;Phobia, Social/diagnosis;Psychometrics/methods;Statistics, Nonparametric},
 pages = {19--40},
 volume = {84},
 number = {1},
 journal = {Psychometrika},
 doi = {10.1007/s11336-018-09658-x},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/30673967},
 file = {Fang2019_Article_OnTheIdentifiabilityOfDiagnost.pdf}
}


@article{GierlAlvesMajeau.2010,
 author = {Gierl, Mark J. and Alves, Cecilia and Majeau, Renate Taylor},
 year = {2010},
 title = {Using the Attribute Hierarchy Method to Make Diagnostic Inferences about Examinees' Knowledge and Skills in Mathematics: An Operational Implementation of Cognitive Diagnostic Assessment},
 pages = {318--341},
 volume = {10},
 number = {4},
 issn = {1530-5058},
 journal = {International Journal of Testing},
 doi = {10.1080/15305058.2010.509554},
 file = {document111.pdf}
}


@article{GierlLeightonHunka.2000,
 author = {Gierl, Mark J. and Leighton, Jacqueline P. and Hunka, Stephen M.},
 year = {2000},
 title = {An NCME Instructional Module on Exploring the Logic of Tatsuoka's Rule-Space Model for Test Development and Analysis},
 pages = {34--44},
 volume = {19},
 number = {3},
 issn = {07311745},
 journal = {Educational Measurement: Issues and Practice},
 doi = {10.1111/j.1745-3992.2000.tb00036.x},
 file = {Leighton - 1996 - Exploring the Logic of Tatsuoka ' s Rule-Space Model for Test Development and Analysis.pdf}
}


@article{Haertel.1989,
 author = {Haertel, Edward H.},
 year = {1989},
 title = {Using Restricted Latent Class Models to Map the Skill Structure of Achievement Items},
 urldate = {2/5/2020},
 pages = {301--321},
 volume = {26},
 number = {4},
 issn = {0022-0655},
 journal = {Journal of Educational Measurement},
 doi = {10.1111/j.1745-3984.1989.tb00336.x},
 file = {Using Restricted Latent Class Models.pdf}
}


@phdthesis{Hartz.2002,
 author = {Hartz, Sarah},
 year = {2002},
 title = {A Bayesian Framework for the Unified Model for Assessing Cognitive Abilities: Blending Theory With Practicality},
 school = {{University of Illinois at Urbana-Champaign}},
 type = {Unpublished doctoral dissertation}
}


@article{HartzRoussos.2008,
 author = {Hartz, Sarah and Roussos, Louis},
 year = {2008},
 title = {THE FUSION MODEL FOR SKILLS DIAGNOSIS: BLENDING THEORY WITH PRACTICALITY},
 keywords = {blocking;formative assessment;fusion model;item response theory;markov chain monte carlo methods;model fit;Q matrix;robustness;simulation;skills diagnosis;stepwise algorithm},
 pages = {i-57},
 volume = {2008},
 number = {2},
 issn = {23308516},
 journal = {ETS Research Report Series},
 doi = {10.1002/j.2333-8504.2008.tb02157.x},
 file = {https://onlinelibrary-wiley-com.libdata.lib.ua.edu/doi/abs/10.1002/j.2333-8504.2008.tb02157.x},
 file = {https://onlinelibrary-wiley-com.libdata.lib.ua.edu/doi/full/10.1002/j.2333-8504.2008.tb02157.x}
}


@article{HensonTemplinWillse.2009,
 author = {Henson, Robert A. and Templin, Jonathan L. and Willse, John T.},
 year = {2009},
 title = {Defining a Family of Cognitive Diagnosis Models Using Log-Linear Models with Latent Variables},
 pages = {191--210},
 volume = {74},
 number = {2},
 journal = {Psychometrika},
 doi = {10.1007/s11336-008-9089-5},
 file = {DEFINING A FAMILY OF COGNITIVE DIAGNOSIS MODELS USING LOG-LINEAR MODELS WITH LATENT VARIABLES.pdf}
}


@article{HongWang.2015,
 abstract = {The issue of latent trait granularity in diagnostic models is considered, comparing and contrasting latent trait and latent class models used for diagnosis. Relationships between conjunctive cognitive diagnosis models (CDMs) with binary attributes and noncompensatory multidimensional item response models are explored, leading to a continuous generalization of the Noisy Input, Deterministic {\textquotedbl}And{\textquotedbl} Gate (NIDA) model. A model that combines continuous and discrete latent variables is proposed that includes a noncompensatory item response theory (IRT) term and a term following the discrete attribute Deterministic Input, Noisy {\textquotedbl}And{\textquotedbl} Gate (DINA) model in cognitive diagnosis. The Tatsuoka fraction subtraction data are analyzed with the proposed models as well as with the DINA model, and classification results are compared. The applicability of the continuous latent trait model and the combined IRT and CDM is discussed, and arguments are given for development of simple models for complex cognitive structures.},
 author = {Hong, Hyokyoung and Wang, Chun and Lim, Youn Seon and Douglas, Jeff},
 year = {2015},
 title = {Efficient Models for Cognitive Diagnosis With Continuous and Mixed-Type Latent Variables},
 keywords = {cognitive diagnosis;multidimensional item response model;noncompensatory item response model},
 pages = {31--43},
 volume = {39},
 number = {1},
 issn = {0146-6216},
 journal = {Applied Psychological Measurement},
 doi = {10.1177/0146621614524981},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/29880992},
 file = {0146621614524981.pdf},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5978571}
}


@article{HuebnerWang.2011,
 abstract = {Educational and Psychological Measurement 2011.71:407-419},
 author = {Huebner, Alan and Wang, Chun},
 year = {2011},
 title = {A Note on Comparing Examinee Classification Methods for Cognitive Diagnosis Models},
 keywords = {classification;cognitive diagnosis;latent class model},
 pages = {407--419},
 volume = {71},
 number = {2},
 issn = {0013-1644},
 journal = {Educational and Psychological Measurement},
 doi = {10.1177/0013164410388832},
 file = {0013164410388832.pdf}
}


@article{HuMiller.2016,
 author = {Hu, Jinxiang and Miller, M. David and Huggins-Manley, Anne Corinne and Chen, Yi-Hsin},
 year = {2016},
 title = {Evaluation of Model Fit in Cognitive Diagnosis Models},
 pages = {119--141},
 volume = {16},
 number = {2},
 issn = {1530-5058},
 journal = {International Journal of Testing},
 doi = {10.1080/15305058.2015.1133627},
 file = {625843.pdf}
}


@phdthesis{Jiang.1996,
 author = {Jiang, Hai},
 year = {1996},
 title = {Applications of computational statistics in cognitive diagnosis and {IRT} modeling},
 school = {{University of Illinois at Urbana-Champaign}},
 file = {Jiang - 1996 - Applications of computational statistics in cognitive diagnosis and IRT modeling.pdf}
}


@article{JiangCarter.2019,
 abstract = {The Bayesian literature has shown that the Hamiltonian Monte Carlo (HMC) algorithm is powerful and efficient for statistical model estimation, especially for complicated models. Stan, a software program built upon HMC, has been introduced as a means of psychometric modeling estimation. However, there are no systemic guidelines for implementing Stan with the log-linear cognitive diagnosis model (LCDM), which is the saturated version of many cognitive diagnostic model (CDM) variants. This article bridges the gap between Stan application and Bayesian LCDM estimation: Both the modeling procedures and Stan code are demonstrated in detail, such that this strategy can be extended to other CDMs straightforwardly.},
 author = {Jiang, Zhehan and Carter, Richard},
 year = {2019},
 title = {Using Hamiltonian Monte Carlo to estimate the log-linear cognitive diagnosis model via Stan},
 keywords = {Algorithms;Bayes Theorem;Bayesian;Cognition;Cognitive diagnostic model;Hamiltonian Monte Carlo (HMC);Humans;LCDM;Linear Models;Markov Chain Monte Carlo (MCMC);Monte Carlo Method;Psychometrics;Software;Stan},
 pages = {651--662},
 volume = {51},
 number = {2},
 journal = {Behavior research methods},
 doi = {10.3758/s13428-018-1069-9},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/29949073},
 file = {Jiang-Carter2019_Article_UsingHamiltonianMonteCarloToEs.pdf}
}


@article{JiangMa.2018,
 abstract = {A log-linear cognitive diagnosis model (LCDM) is estimated via a global optimization approach- differential evolution optimization (DEoptim), which can be used when the traditional expectation maximization (EM) fails. The application of the DEoptim to LCDM estimation is introduced, explicated, and evaluated via a Monte Carlo simulation study in this article. The aim of this study is to fill the gap between the field of psychometric modeling and modern machine learning estimation techniques and provide an alternative solution in the model estimation.},
 author = {Jiang, Zhehan and Ma, Wenchao},
 year = {2018},
 title = {Integrating Differential Evolution Optimization to Cognitive Diagnostic Model Estimation},
 keywords = {Cognitive diagnostic model;Differential Evolution Optimization;EM algorithm;estimation;LCDM},
 pages = {2142},
 volume = {9},
 issn = {1664-1078},
 journal = {Frontiers in Psychology},
 doi = {10.3389/fpsyg.2018.02142},
 file = {Jiang, Ma 2018 - Integrating Differential Evolution Optimization.pdf}
}


@article{JohnsonSinharay.2018,
 abstract = {Journal of Educational Measurement 2018.55:635-664},
 author = {Johnson, Matthew S. and Sinharay, Sandip},
 year = {2018},
 title = {Measures of Agreement to Assess Attribute-Level Classification Accuracy and Consistency for Cognitive Diagnostic Assessments},
 pages = {635--664},
 volume = {55},
 number = {4},
 issn = {0022-0655},
 journal = {Journal of Educational Measurement},
 doi = {10.1111/jedm.12196},
 file = {jedm.12196(2).pdf}
}


@article{JurichBradshaw.2014,
 author = {Jurich, Daniel P. and Bradshaw, Laine P.},
 year = {2014},
 title = {An Illustration of Diagnostic Classification Modeling in Student Learning Outcomes Assessment},
 pages = {49--72},
 volume = {14},
 number = {1},
 issn = {1530-5058},
 journal = {International Journal of Testing},
 doi = {10.1080/15305058.2013.835728},
 file = {Jurich, Bradshaw - 2014 - An Illustration of Diagnostic Classification Modeling in Student Learning Outcomes Assessment.pdf}
}


@article{KabiriGhaziTabatabaei.2017,
 author = {Kabiri, Masoud and Ghazi-Tabatabaei, Mahmood and Bazargan, Abbas and Shokoohi-Yekta, Mohsen and Kharrazi, Kamal},
 year = {2017},
 title = {Diagnosing Competency Mastery in Science: An Application of {GDM} to {TIMSS} 2011 Data},
 pages = {27--38},
 volume = {30},
 number = {1},
 issn = {0895-7347},
 journal = {Applied Measurement in Education},
 doi = {10.1080/08957347.2016.1258407},
 file = {Kabiri et al. - 2017 - Diagnosing Competency Mastery in Science An Application of GDM to TIMSS 2011 Data.pdf}
}


@article{KaplanLaTorreBarrada.2015,
 abstract = {This article introduces two new item selection methods, the modified posterior-weighted Kullback-Leibler index (MPWKL) and the generalized deterministic inputs, noisy {\textquotedbl}and{\textquotedbl} gate (G-DINA) model discrimination index (GDI), that can be used in cognitive diagnosis computerized adaptive testing. The efficiency of the new methods is compared with the posterior-weighted Kullback-Leibler (PWKL) item selection index using a simulation study in the context of the G-DINA model. The impact of item quality, generating models, and test termination rules on attribute classification accuracy or test length is also investigated. The results of the study show that the MPWKL and GDI perform very similarly, and have higher correct attribute classification rates or shorter mean test lengths compared with the PWKL. In addition, the GDI has the shortest implementation time among the three indices. The proportion of item usage with respect to the required attributes across the different conditions is also tracked and discussed.},
 author = {Kaplan, Mehmet and de {La Torre}, Jimmy and Barrada, Juan Ram{\'o}n},
 year = {2015},
 title = {New Item Selection Methods for Cognitive Diagnosis Computerized Adaptive Testing},
 urldate = {2/23/2020},
 pages = {167--188},
 volume = {39},
 number = {3},
 issn = {0146-6216},
 journal = {Applied Psychological Measurement},
 doi = {10.1177/0146621614554650},
 file = {d55761e3-dff4-4179-8d62-b7530afbd202:C\:\\Users\\wench\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\hzsl0hndqon2ipk638qbntgdllf1r1q5dxzhi26xkkmxy8hcs6\\Citavi Attachments\\d55761e3-dff4-4179-8d62-b7530afbd202.pdf:pdf}
}


@article{KohnChiu.2017,
 abstract = {The Q-matrix of a cognitively diagnostic test is said to be complete if it allows for the identification of all possible proficiency classes among examinees. Completeness of the Q-matrix is therefore a key requirement for any cognitively diagnostic test. However, completeness of the Q-matrix is often difficult to establish, especially, for tests with a large number of items involving multiple attributes. As an additional complication, completeness is not an intrinsic property of the Q-matrix, but can only be assessed in reference to a specific cognitive diagnosis model (CDM) supposed to underly the data-that is, the Q-matrix of a given test can be complete for one model but incomplete for another. In this article, a method is presented for assessing whether a given Q-matrix is complete for a given CDM. The proposed procedure relies on the theoretical framework of general CDMs and is therefore legitimate for CDMs that can be reparameterized as a general CDM.},
 author = {K{\"o}hn, Hans-Friedrich and Chiu, Chia-Yi},
 year = {2017},
 title = {A Procedure for Assessing the Completeness of the {Q}-Matrices of Cognitively Diagnostic Tests},
 pages = {112--132},
 volume = {82},
 number = {1},
 journal = {Psychometrika},
 doi = {10.1007/s11336-016-9536-7},
 file = {Köhn, Chiu - 2017 - A Procedure for Assessing the Completeness of the Q-Matrices of Cognitively Diagnostic Tests.pdf},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/27714544}
}


@article{KuninaHabenichtRuppWilhelm.2009,
 author = {Kunina-Habenicht, Olga and Rupp, Andr{\'e} A. and Wilhelm, Oliver},
 year = {2009},
 title = {A practical illustration of multidimensional diagnostic skills profiling: Comparing results from confirmatory factor analysis and diagnostic classification models},
 pages = {64--70},
 volume = {35},
 number = {2-3},
 issn = {0191491X},
 journal = {Studies in Educational Evaluation},
 doi = {10.1016/j.stueduc.2009.10.003},
 file = {1-s2.0-S0191491X09000224-main.pdf}
}


@article{KuninaHabenichtRuppWilhelm.2017,
 author = {Kunina-Habenicht, Olga and Rupp, Andr{\'e} A. and Wilhelm, Oliver},
 year = {2017},
 title = {Incremental Validity of Multidimensional Proficiency Scores from Diagnostic Classification Models: An Illustration for Elementary School Mathematics},
 pages = {277--301},
 volume = {17},
 number = {4},
 issn = {1530-5058},
 journal = {International Journal of Testing},
 doi = {10.1080/15305058.2017.1291517},
 file = {Kunina-Habenicht, Rupp, Wilhelm - 2017 - Incremental Validity of Multidimensional Proficiency Scores from Diagnostic Classification Mode.pdf}
}


@article{LeeParkTaylan.2011,
 author = {Lee, Young-Sun and Park, Yoon Soo and Taylan, Didem},
 year = {2011},
 title = {A Cognitive Diagnostic Modeling of Attribute Mastery in {Massachusetts, Minnesota}, and the {U.S.} National Sample Using the {TIMSS} 2007},
 pages = {144--177},
 volume = {11},
 number = {2},
 issn = {1530-5058},
 journal = {International Journal of Testing},
 doi = {10.1080/15305058.2010.534571},
 file = {Lee, Park, Taylan - 2011 - A Cognitive Diagnostic Modeling of Attribute Mastery in Massachusetts, Minnesota, and the U.S. National Sampl.pdf}
}


@article{LeeSawaki.2009,
 author = {Lee, Yong-Won and Sawaki, Yasuyo},
 year = {2009},
 title = {Cognitive Diagnosis Approaches to Language Assessment: An Overview},
 pages = {172--189},
 volume = {6},
 number = {3},
 issn = {1543-4303},
 journal = {Language Assessment Quarterly},
 doi = {10.1080/15434300902985108},
 file = {Overview.pdf}
}


@article{LeeSawaki.2009b,
 author = {Lee, Yong-Won and Sawaki, Yasuyo},
 year = {2009},
 title = {Application of Three Cognitive Diagnosis Models to ESL Reading and Listening Assessments},
 pages = {239--263},
 volume = {6},
 number = {3},
 issn = {1543-4303},
 journal = {Language Assessment Quarterly},
 doi = {10.1080/15434300903079562},
 file = {Application_CDAModels.pdf}
}


@book{LeightonGierl.2007b,
 author = {Leighton, Jacqueline P. and Gierl, Mark J.},
 year = {2007},
 title = {Cognitive diagnostic assessment for education: Theory and applications},
 keywords = {Cognition   Testing;Educational tests and measurements},
 address = {Cambridge},
 publisher = {{Cambridge University Press}},
 isbn = {0521684218},
 file = {http://www.loc.gov/catdir/enhancements/fy0729/2006035160-b.html},
 file = {http://www.loc.gov/catdir/toc/ecip074/2006035160.html}
}


@article{LeightonGierlHunka.2004,
 author = {Leighton, Jacqueline P. and Gierl, Mark J. and Hunka, Stephen M.},
 year = {2004},
 title = {The Attribute Hierarchy Method for Cognitive Assessment: A Variation on Tatsuoka's Rule-Space Approach},
 pages = {205--237},
 volume = {41},
 number = {3},
 issn = {0022-0655},
 journal = {Journal of Educational Measurement},
 doi = {10.1111/j.1745-3984.2004.tb01163.x},
 file = {Leighton, Gierl, Hunka - 2004 - The Attribute Hierarchy Method for Cognitive Assessment A Variation on Tatsuoka's Rule-Space Approach.pdf}
}


@article{LimDrasgow.2017,
 abstract = {A nonparametric technique based on the Hamming distance is proposed in this research by recognizing that once the attribute vector is known, or correctly estimated with high probability, one can determine the item-by-attribute vectors for new items undergoing calibration. We consider the setting where Q is known for a large item bank, and the q-vectors of additional items are estimated. The method is studied in simulation under a wide variety of conditions, and is illustrated with the Tatsuoka fraction subtraction data. A consistency theorem is developed giving conditions under which nonparametric Q calibration can be expected to work.},
 author = {Lim, Youn Seon and Drasgow, Fritz},
 year = {2017},
 title = {Nonparametric Calibration of Item-by-Attribute Matrix in Cognitive Diagnosis},
 keywords = {Calibration;Cognition/classification;cognitive diagnosis;Data Interpretation, Statistical;nonparametric classification;online calibration;Psychometrics/methods;Statistics, Nonparametric},
 pages = {562--575},
 volume = {52},
 number = {5},
 journal = {Multivariate behavioral research},
 doi = {10.1080/00273171.2017.1341829},
 file = {Lim, Drasgow - 2017 - Nonparametric Calibration of Item-by-Attribute Matrix in Cognitive Diagnosis.pdf},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/28715230}
}


@article{LiuDouglasHenson.2009,
 author = {Liu, Ying and Douglas, Jeffrey A. and Henson, Robert A.},
 year = {2009},
 title = {Testing Person Fit in Cognitive Diagnosis},
 pages = {579--598},
 volume = {33},
 number = {8},
 issn = {0146-6216},
 journal = {Applied Psychological Measurement},
 doi = {10.1177/0146621609331960},
 file = {Liu, Douglas, Henson - 2009 - Testing person fit in cognitive diagnosis.pdf}
}


@article{LiuJiang.2018,
 abstract = {The purpose of this study is to develop and evaluate two diagnostic classification models (DCMs) for scoring ordinal item data. We first applied the proposed models to an operational dataset and compared their performance to an epitome of current polytomous DCMs in which the ordered data structure is ignored. Findings suggest that the much more parsimonious models that we proposed performed similarly to the current polytomous DCMs and offered useful item-level information in addition to option-level information. We then performed a small simulation study using the applied study condition and demonstrated that the proposed models can provide unbiased parameter estimates and correctly classify individuals. In practice, the proposed models can accommodate much smaller sample sizes than current polytomous DCMs and thus prove useful in many small-scale testing scenarios.},
 author = {Liu, Ren and Jiang, Zhehan},
 year = {2018},
 title = {Diagnostic Classification Models for Ordinal Item Responses},
 keywords = {Bayesian estimation;diagnostic classification model;Markov Chain Monte Carlo (MCMC);ordinal item responses;partial credit model;rating scales},
 pages = {2512},
 volume = {9},
 issn = {1664-1078},
 journal = {Frontiers in psychology},
 doi = {10.3389/fpsyg.2018.02512},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/30618941},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6297886},
 file = {fpsyg-09-02512.pdf}
}


@article{LiuTianXin.2016,
 author = {Liu, Yanlou and Tian, Wei and Xin, Tao},
 year = {2016},
 title = {An Application of $M_2$ Statistic to Evaluate the Fit of Cognitive Diagnostic Models},
 pages = {3--26},
 volume = {41},
 number = {1},
 issn = {1076-9986},
 journal = {Journal of Educational and Behavioral Statistics},
 doi = {10.3102/1076998615621293},
 file = {Liu, Tian, Xin - 2015 - An Application of M2 Statistic to Evaluate the Fit of Cognitive Diagnostic Models.pdf}
}


@article{LiuWu.2018,
 author = {Liu, Qi and Wu, Runze and Chen, Enhong and Xu, Guandong and Su, Yu and Chen, Zhigang and Hu, Guoping},
 year = {2018},
 title = {Fuzzy Cognitive Diagnosis for Modelling Examinee Performance},
 pages = {1--26},
 volume = {9},
 number = {4},
 issn = {21576904},
 journal = {ACM Transactions on Intelligent Systems and Technology},
 doi = {10.1145/3168361},
 file = {Liu et al. - 2018 - Fuzzy Cognitive Diagnosis for Modelling Examinee.pdf}
}


@article{LiuXuYing.2012,
 abstract = {The recent surge of interests in cognitive assessment has led to developments of novel statistical models for diagnostic classification. Central to many such models is the well-known Q-matrix, which specifies the item-attribute relationships. This article proposes a data-driven approach to identification of the Q-matrix and estimation of related model parameters. A key ingredient is a flexible T-matrix that relates the Q-matrix to response patterns. The flexibility of the T-matrix allows the construction of a natural criterion function as well as a computationally amenable algorithm. Simulations results are presented to demonstrate usefulness and applicability of the proposed method. Extension to handling of the Q-matrix with partial information is presented. The proposed method also provides a platform on which important statistical issues, such as hypothesis testing and model selection, may be formally addressed.},
 author = {Liu, Jingchen and Xu, Gongjun and Ying, Zhiliang},
 year = {2012},
 title = {Data-Driven Learning of {Q}-Matrix},
 pages = {548--564},
 volume = {36},
 number = {7},
 issn = {0146-6216},
 journal = {Applied Psychological Measurement},
 doi = {10.1177/0146621612456591},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/23926363},
 file = {Liu, Xu, Ying - 2012 - Data-Driven Learning of Q-Matrix.pdf},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3733574}
}


@article{Ma.2019,
 abstract = {Applied Psychological Measurement 0.0:0146621619843829},
 author = {Ma, Wenchao},
 year = {2019},
 title = {Evaluating the Fit of Sequential {G-DINA} Model Using Limited-Information Measures},
 keywords = {cognitive diagnosis;goodness-of-fit;limited information;model-data fit;ordinal response;sequential G-DINA},
 pages = {014662161984382},
 issn = {0146-6216},
 journal = {Applied Psychological Measurement},
 doi = {10.1177/0146621619843829},
 file = {8ac7bff3-935f-4d50-9981-0853e0ecdf84:C\:\\Users\\wench\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\hzsl0hndqon2ipk638qbntgdllf1r1q5dxzhi26xkkmxy8hcs6\\Citavi Attachments\\8ac7bff3-935f-4d50-9981-0853e0ecdf84.pdf:pdf}
}


@article{Ma.2019b,
 abstract = {Constructed-response items have been shown to be appropriate for cognitively diagnostic assessments because students' problem-solving procedures can be observed, providing direct evidence for making inferences about their proficiency. However, multiple strategies used by students make item scoring and psychometric analyses challenging. This study introduces the so-called two-digit scoring scheme into diagnostic assessments to record both students' partial credits and their strategies. This study also proposes a diagnostic tree model (DTM) by integrating the cognitive diagnosis models with the tree model to analyse the items scored using the two-digit rubrics. Both convergent and divergent tree structures are considered to accommodate various scoring rules. The MMLE/EM algorithm is used for item parameter estimation of the DTM, and has been shown to provide good parameter recovery under varied conditions in a simulation study. A set of data from TIMSS 2007 mathematics assessment is analysed to illustrate the use of the two-digit scoring scheme and the DTM.},
 author = {Ma, Wenchao},
 year = {2019},
 title = {A diagnostic tree model for polytomous responses with multiple strategies},
 keywords = {Algorithms;Canada;Cognition;Computer Simulation;Educational Measurement/methods;Humans;Mathematics;Models, Statistical;Problem Solving;Psychometrics/methods;Students;United States},
 pages = {61--82},
 volume = {72},
 number = {1},
 issn = {0007-1102},
 journal = {The British journal of mathematical and statistical psychology},
 doi = {10.1111/bmsp.12137},
 file = {4f72c843-b492-4461-9d44-aaf3f29bc490:C\:\\Users\\wench\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\hzsl0hndqon2ipk638qbntgdllf1r1q5dxzhi26xkkmxy8hcs6\\Citavi Attachments\\4f72c843-b492-4461-9d44-aaf3f29bc490.pdf:pdf}
}


@article{MacreadyDayton.1977,
 author = {Macready, George B. and Dayton, C. Mitchell},
 year = {1977},
 title = {The Use of Probabilistic Models in the Assessment of Mastery},
 pages = {99--120},
 volume = {2},
 number = {2},
 issn = {0362-9791},
 journal = {Journal of Educational Statistics},
 doi = {10.3102/10769986002002099},
 file = {Macready et al. - 2015 - The Use of Probabilistic Models in the Assessment of Mastery Linked references are available on JSTOR for this.pdf}
}


@article{MadelaTorre.2016,
 abstract = {This paper proposes a general polytomous cognitive diagnosis model for a special type of graded responses, where item categories are attained in a sequential manner, and associated with some attributes explicitly. To relate categories to attributes, a category-level Q-matrix is used. When the attribute and category association is specified a priori, the proposed model has the flexibility to allow different cognitive processes (e.g., conjunctive, disjunctive) to be modelled at different categories within a single item. This model can be extended for items where categories cannot be explicitly linked to attributes, and for items with unordered categories. The feasibility of the proposed model is examined using simulated data. The proposed model is illustrated using the data from the Trends in International Mathematics and Science Study 2007 assessment.},
 author = {Ma, Wenchao and {de la Torre}, Jimmy},
 year = {2016},
 title = {A sequential cognitive diagnosis model for polytomous responses},
 keywords = {Algorithms;Cognition Disorders/diagnosis;Computer Simulation;Data Interpretation, Statistical;Educational Measurement/methods;Humans;Models, Statistical;Psychometrics/methods;Reproducibility of Results;Sensitivity and Specificity;Surveys and Questionnaires},
 pages = {253--275},
 volume = {69},
 number = {3},
 issn = {0007-1102},
 journal = {The British journal of mathematical and statistical psychology},
 doi = {10.1111/bmsp.12070},
 file = {Ma, de la Torre - 2016 - A sequential cognitive diagnosis model for polytomous responses.pdf},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/27317397}
}


@article{MadelaTorre.2019,
 author = {Ma, Wenchao and {de la Torre}, Jimmy},
 year = {2019},
 title = {Digital Module 05: Diagnostic Measurement---The {G-DINA} Framework https://ncme.elevate.commpartners.com},
 pages = {114--115},
 volume = {38},
 number = {2},
 issn = {07311745},
 journal = {Educational Measurement: Issues and Practice},
 doi = {10.1111/emip.12262},
 file = {Ma_et_al-2019-Educational_Measurement__Issues_and_Practice.pdf}
}


@article{MadelaTorre.2019b,
 abstract = {Journal of Educational and Behavioral Statistics 0.0:1076998618792484},
 author = {Ma, Wenchao and {de la Torre}, Jimmy},
 year = {2019},
 title = {Category-Level Model Selection for the Sequential {G-DINA} Model},
 keywords = {cognitive diagnosis;G-DINA;model selection;polytomous data;sequential CDM},
 pages = {45--77},
 volume = {44},
 number = {1},
 issn = {1076-9986},
 journal = {Journal of Educational and Behavioral Statistics},
 doi = {10.3102/1076998618792484},
 file = {1076998618792484.pdf}
}


@article{MadisonBradshaw.2018,
 abstract = {A common assessment research design is the single-group pre-test/post-test design in which examinees are administered an assessment before instruction and then another assessment after instruction. In this type of study, the primary objective is to measure growth in examinees, individually and collectively. In an item response theory (IRT) framework, longitudinal IRT models can be used to assess growth in examinee ability over time. In a diagnostic classification model (DCM) framework, assessing growth translates to measuring changes in attribute mastery status over time, thereby providing a categorical, criterion-referenced interpretation of growth. This study introduces the Transition Diagnostic Classification Model (TDCM), which combines latent transition analysis with the log-linear cognitive diagnosis model to provide methodology for analyzing growth in a general DCM framework. Simulation study results indicate that the proposed model is flexible, provides accurate and reliable classifications, and is quite robust to violations to measurement invariance over time. The TDCM is used to analyze pre-test/post-test data from a diagnostic mathematics assessment.},
 author = {Madison, Matthew J. and Bradshaw, Laine P.},
 year = {2018},
 title = {Assessing Growth in a Diagnostic Classification Model Framework},
 keywords = {Academic Success;Cognitive diagnosis model;Computer Simulation;Data Interpretation, Statistical;diagnostic classification model;diagnostic classification model,cognitive diagnosis model,latent transition analysis,item parameter drift,measurement invariance,growth,pre-test/post-test design;growth;Humans;item parameter drift;latent transition analysis;Longitudinal Studies;Mathematical Concepts;measurement invariance;Models, Theoretical;pre-test/post-test design;Psychometrics/methods;Students},
 urldate = {2/7/2020},
 pages = {963--990},
 volume = {83},
 number = {4},
 journal = {Psychometrika},
 doi = {10.1007/s11336-018-9638-5},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/30264183},
 file = {Madison, Bradshaw 2018 - Assessing Growth in a Diagnostic.pdf}
}


@article{MaGuo.2019,
 abstract = {Cognitive diagnosis models (CDMs) have been used as psychometric tools in educational assessments to estimate students' proficiency profiles. However, most CDMs assume that all students adopt the same strategy when approaching problems in an assessment, which may not be the case in practice. This study develops a generalized multiple-strategy CDM for dichotomous response data. The proposed model provides a unified framework to accommodate various condensation rules (e.g., conjunctive, disjunctive, and additive) and different strategy selection approaches (i.e., probability-matching, over-matching, and maximizing). Model parameters are estimated using the marginal maximum likelihood estimation via expectation-maximization algorithm. Simulation studies showed that the parameters of the proposed model can be adequately recovered and that the proposed model was relatively robust to some types of model misspecifications. A set of real data was analysed as well to illustrate the use of the proposed model in practice.},
 author = {Ma, Wenchao and Guo, Wenjing},
 year = {2019},
 title = {Cognitive diagnosis models for multiple strategies},
 keywords = {Algorithms;Cognition;Computer Simulation;Educational Measurement/methods;Humans;Likelihood Functions;Psychometrics/methods},
 pages = {370--392},
 volume = {72},
 number = {2},
 issn = {0007-1102},
 journal = {The British journal of mathematical and statistical psychology},
 doi = {10.1111/bmsp.12155},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/30756389},
 file = {Ma_et_al-2019-British_Journal_of_Mathematical_and_Statistical_Psychology(1).pdf}
}


@article{MaIaconangelodelaTorre.2016,
 abstract = {Selecting the most appropriate cognitive diagnosis model (CDM) for an item is a challenging process. Although general CDMs provide better model-data fit, specific CDMs have more straightforward interpretations, are more stable, and can provide more accurate classifications when used correctly. Recently, the Wald test has been proposed to determine at the item level whether a general CDM can be replaced by specific CDMs without a significant loss in model-data fit. The current study examines the practical consequence of the test by evaluating whether the attribute-vector classification based on CDMs selected by the Wald test is better than that based on general CDMs. Although the Wald test can detect the true underlying model for certain CDMs, it is yet unclear how effective it is at distinguishing among the wider range of CDMs found in the literature. This study investigates the relative similarity of the various CDMs through the use of the newly developed dissimiliarity index, and explores the implications for the Wald test. Simulations show that the Wald test cannot distinguish among additive models due to their inherent similarity, but this does not impede the ability of the test to provide higher correct classification rates than general CDMs, particularly when the sample size is small and items are of low quality. An empirical example is included to demonstrate the viability of the procedure.},
 author = {Ma, Wenchao and Iaconangelo, Charles and {de la Torre}, Jimmy},
 year = {2016},
 title = {Model Similarity, Model Selection, and Attribute Classification},
 pages = {200--217},
 volume = {40},
 number = {3},
 issn = {0146-6216},
 journal = {Applied Psychological Measurement},
 doi = {10.1177/0146621615621717},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5978484},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/29881048},
 file = {Ma, Iaconangelo, de la Torre - 2016 - Model Similarity, Model Selection, and Attribute Classification.pdf}
}


@article{MaLaTorre.2020,
 abstract = {As a core component of most cognitive diagnosis models, the Q-matrix, or item and attribute association matrix, is typically developed by domain experts, and tends to be subjective. It is critical to validate the Q-matrix empirically because a misspecified Q-matrix could result in erroneous attribute estimation. Most existing Q-matrix validation procedures are developed for dichotomous responses. However, in this paper, we propose a method to empirically detect and correct the misspecifications in the Q-matrix for graded response data based on the sequential generalized deterministic inputs, noisy 'and' gate (G-DINA) model. The proposed Q-matrix validation procedure is implemented in a stepwise manner based on the Wald test and an effect size measure. The feasibility of the proposed method is examined using simulation studies. Also, a set of data from the Trends in International Mathematics and Science Study (TIMSS) 2011 mathematics assessment is analysed for illustration.},
 author = {Ma, Wenchao and de {La Torre}, Jimmy},
 year = {2020},
 title = {An empirical {Q}-matrix validation method for the sequential generalized {DINA} model},
 urldate = {2/23/2020},
 pages = {142--163},
 volume = {73},
 number = {1},
 issn = {0007-1102},
 journal = {The British journal of mathematical and statistical psychology},
 doi = {10.1111/bmsp.12156},
 file = {c00e3710-6d11-4877-9a03-f73f9b3d718c:C\:\\Users\\wench\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\hzsl0hndqon2ipk638qbntgdllf1r1q5dxzhi26xkkmxy8hcs6\\Citavi Attachments\\c00e3710-6d11-4877-9a03-f73f9b3d718c.pdf:pdf}
}


@article{MaMeng.2014,
 author = {Ma, Xiaomei and Meng, Yaru},
 year = {2014},
 title = {Towards Personalized English Learning Diagnosis Cognitive Diagnostic Modelling for EFL Listening},
 pages = {336--348},
 volume = {2},
 number = {5},
 journal = {Asian Journal of Education and e-Learning},
 file = {Ma, Meng - 2014 - Towards Personalized English Learning Diagnosis Cognitive Diagnostic Modelling for {EFL} Listening.pdf}
}


@article{Maris.1999,
 author = {Maris, E.},
 year = {1999},
 title = {Estimating multiple classification latent class models},
 pages = {187--212},
 volume = {64},
 number = {2},
 journal = {Psychometrika},
 doi = {10.1007/bf02294535},
 file = {Unknown - 1999 - multiple latent classes.pdf}
}


@article{NajeraSorrelAbad.2019,
 abstract = {Educational and Psychological Measurement 0.0:0013164418822700},
 author = {N{\'a}jera, Pablo and Sorrel, Miguel A. and Abad, Francisco Jos{\'e}},
 year = {2019},
 title = {Reconsidering Cutoff Points in the General Method of Empirical {Q}-Matrix Validation},
 keywords = {CDM;GDI;G-DINA;Q-matrix;validation},
 pages = {727--753},
 volume = {79},
 number = {4},
 issn = {0013-1644},
 journal = {Educational and Psychological Measurement},
 doi = {10.1177/0013164418822700},
 file = {0013164418822700.pdf}
}


@article{Nichols.1994,
 author = {Nichols, Paul D.},
 year = {1994},
 title = {A Framework for Developing Cognitively Diagnostic Assessments},
 pages = {575--603},
 volume = {64},
 number = {4},
 issn = {0034-6543},
 journal = {Review of Educational Research},
 doi = {10.3102/00346543064004575},
 file = {Nichols 1994 - A Framework for Developing Cognitively.pdf}
}


@book{NicholsChipmanBrennan.1995,
 year = {1995},
 title = {Cognitively Diagnostic Assessment},
 publisher = {{Hills-dale, NJ:Erlbaum}},
 editor = {Nichols, Paul D. and Chipman, S. F. and Brennan, Robert L.}
}


@article{ParkXingLee.2018,
 abstract = {Large-scale educational testing data often contain vast amounts of variables associated with information pertaining to test takers, schools, or access to educational resources-information that can help explain relationships between test taker performance and their learning environment. This study examines approaches to incorporate latent and observed explanatory variables as predictors for cognitive diagnostic models (CDMs). Methods to specify and simultaneously estimate observed and latent variables (estimated using item response theory) as predictors affecting attribute mastery were examined. Real-world data analyses were conducted to demonstrate the application using large-scale international testing data. Simulation studies were conducted to examine the recovery and classification for simultaneously estimating multiple latent (using dichotomous and polytomous items as indicators for the latent construct) and observed predictors for varying sample sizes and number of attributes. Results showed stable parameter recovery and consistency in attribute classification. Implications for latent predictors and attribute specifications are discussed.},
 author = {Park, Yoon Soo and Xing, Kuan and Lee, Young-Sun},
 year = {2018},
 title = {Explanatory Cognitive Diagnostic Models: Incorporating Latent and Observed Predictors},
 keywords = {cognitive diagnostic models;covariates;explanatory models;item response theory},
 pages = {376--392},
 volume = {42},
 number = {5},
 issn = {0146-6216},
 journal = {Applied Psychological Measurement},
 doi = {10.1177/0146621617738012},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/30034055},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6023094},
 file = {Park, Xing, Lee - 2017 - Explanatory Cognitive Diagnostic Models Incorporating Latent and Observed Predictors.pdf}
}


@book{RuppLeighton.2017,
 year = {2016},
 title = {The handbook of cognition and assessment: Frameworks, methodologies, and applications},
 keywords = {Cognitive learning;Educational evaluation;Handbooks, manuals, etc},
 address = {Chichester UK and Hoboken NJ},
 publisher = {{Wiley Blackwell}},
 isbn = {9781118956618},
 editor = {Rupp, Andr{\'e} A. and Leighton, Jacqueline P.}
}


@article{RuppTemplin.2008,
 author = {Rupp, Andr{\'e} A. and Templin, Jonathan L.},
 year = {2008},
 title = {Unique Characteristics of Diagnostic Classification Models: A Comprehensive Review of the Current State-of-the-Art},
 pages = {219--262},
 volume = {6},
 number = {4},
 issn = {1536-6367},
 journal = {Measurement: Interdisciplinary Research {\&} Perspective},
 doi = {10.1080/15366360802490866},
 file = {Rupp, Templin - 2008 - Unique characteristics of diagnostic classification models A comprehensive review of the current state-of-the-art.pdf}
}


@book{RuppTemplinHenson.2010,
 author = {Rupp, Andr{\'e} A. and Templin, Jonathan and Henson, Robert A.},
 year = {2010},
 title = {Diagnostic measurement: Theory, methods, and applications},
 price = {{\pounds}33.00},
 address = {New York and London},
 publisher = {Guilford},
 isbn = {9781606235270},
 series = {Methodology in the social sciences},
 file = {Ch6 of Diagnostic_Measurement.pdf}
}


@article{SantosLaTorreDavier.2019,
 abstract = {J Classif, doi:10.1007/s00357-019-09325-5},
 author = {Santos, Kevin Carl P. and de {La Torre}, Jimmy and von Davier, Matthias},
 year = {2019},
 title = {Adjusting Person Fit Index for Skewness in Cognitive Diagnosis Modeling},
 keywords = {Aberrant response patterns;cognitive diagnosis models;Cornish-Fisher expansion;Edgeworth expansion;Person fit;{\th}ÿ;\textgreek{q}2-approximation},
 urldate = {2/23/2020},
 pages = {191},
 volume = {62},
 issn = {0176-4268},
 journal = {Journal of Classification},
 doi = {10.1007/s00357-019-09325-5},
 file = {625b8638-cf3f-4ec7-a715-9c055d9f3851:C\:\\Users\\wench\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\hzsl0hndqon2ipk638qbntgdllf1r1q5dxzhi26xkkmxy8hcs6\\Citavi Attachments\\625b8638-cf3f-4ec7-a715-9c055d9f3851.pdf:pdf}
}


@incollection{SeipelBiancarosa.2018,
 author = {Seipel, Ben and Biancarosa, Gina and Carlson, Sarah E. and Davison, Mark L.},
 title = {The Need, Use, and Future of Cognitive Diagnostic Assessments in Classroom Practice},
 pages = {1--23},
 volume = {2},
 publisher = {{Information Science Reference}},
 isbn = {9781522531326},
 series = {Advances in Educational Technologies and Instructional Design},
 editor = {Wang, Victor C. X.},
 booktitle = {Handbook of research on program development and assessment methodologies in K-20 education},
 year = {2018},
 address = {Hershey PA},
 doi = {10.4018/978-1-5225-3132-6.ch001}
}


@article{SenBradshaw.2017,
 abstract = {The purpose of this study was to thoroughly examine the performance of three information-based fit indices-Akaike's Information Criterion (AIC), Bayesian Information Criterion (BIC), and sample-size-adjusted BIC (SABIC)-using the log-linear cognitive diagnosis model and a set of well-known item response theory (IRT) models. Two simulation studies were conducted to examine the extent to which relative fit indices can identify the generating model under a variety of data conditions and model misspecifications. Generally, indices performed better when item quality was stronger. When the IRT was the generating model, all three indices correctly selected the IRT model for all replications. When the true model was a diagnostic classification model, for all three fit indices, the multidimensional IRT model was incorrectly selected as frequently as 70{\%} of the replications. The results of this study identify situations for researchers where commonly used-and typically well-performing-fit indices may not be appropriate to compare models for selection.},
 author = {Sen, Sedat and Bradshaw, Laine},
 year = {2017},
 title = {Comparison of Relative Fit Indices for Diagnostic Model Selection},
 pages = {422--438},
 volume = {41},
 number = {6},
 issn = {0146-6216},
 journal = {Applied Psychological Measurement},
 doi = {10.1177/0146621617695521},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5978522},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/29881100},
 file = {10.1177_0146621617695521.pdf}
}


@article{SessomsHenson.2018,
 author = {Sessoms, John and Henson, Robert A.},
 year = {2018},
 title = {Applications of Diagnostic Classification Models: A Literature Review and Critical Commentary},
 keywords = {applications;cognitive diagnosis models;cognitive diagnostic models;Diagnostic classification models},
 pages = {1--17},
 volume = {16},
 number = {1},
 issn = {1536-6367},
 journal = {Measurement: Interdisciplinary Research and Perspectives},
 doi = {10.1080/15366367.2018.1435104},
 file = {Sessoms, Henson - 2018 - Applications of Diagnostic Classification Models A Literature Review and Critical Commentary.pdf}
}


@article{SinharayAlmond.2007,
 author = {Sinharay, Sandip and Almond, Russell G.},
 year = {2007},
 title = {Assessing Fit of Cognitive Diagnostic Models A Case Study},
 pages = {239--257},
 volume = {67},
 number = {2},
 issn = {0013-1644},
 journal = {Educational and Psychological Measurement},
 doi = {10.1177/0013164406292025},
 file = {Sinharay, Almond - 2007 - Assessing Fit of Cognitive Diagnostic Models A Case Study.pdf}
}


@article{SkaggsWilkinsHein.2016,
 author = {Skaggs, Gary and Wilkins, Jesse L. M. and Hein, Serge F.},
 year = {2016},
 title = {Grain Size and Parameter Recovery with TIMSS and the General Diagnostic Model},
 pages = {310--330},
 volume = {16},
 number = {4},
 issn = {1530-5058},
 journal = {International Journal of Testing},
 doi = {10.1080/15305058.2016.1145683},
 file = {Skaggs, Wilkins, Hein - 2016 - Grain Size and Parameter Recovery with TIMSS and the General Diagnostic Model.pdf}
}


@article{Sorrel.2017,
 author = {Sorrel, Miguel A. and de {La Torre}, Jimmy and Abad, Francisco J. and Olea, Julio},
 year = {2017},
 title = {Two-Step Likelihood Ratio Test for Item-Level Model Comparison in Cognitive Diagnosis Models},
 urldate = {2/19/2020},
 pages = {39--47},
 volume = {13},
 number = {Supplement 1},
 issn = {1614-1881},
 journal = {Methodology},
 doi = {10.1027/1614-2241/a000131},
 file = {7566a85c-79ab-4cde-854f-3adae3be9c29:C\:\\Users\\wench\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\hzsl0hndqon2ipk638qbntgdllf1r1q5dxzhi26xkkmxy8hcs6\\Citavi Attachments\\7566a85c-79ab-4cde-854f-3adae3be9c29.pdf:pdf}
}


@article{SorrelAbad.2017,
 abstract = {Research related to the fit evaluation at the item level involving cognitive diagnosis models (CDMs) has been scarce. According to the parsimony principle, balancing goodness of fit against model complexity is necessary. General CDMs require a larger sample size to be estimated reliably, and can lead to worse attribute classification accuracy than the appropriate reduced models when the sample size is small and the item quality is poor, which is typically the case in many empirical applications. The main purpose of this study was to systematically examine the statistical properties of four inferential item-fit statistics: S-X2 , the likelihood ratio (LR) test, the Wald (W) test, and the Lagrange multiplier (LM) test. To evaluate the performance of the statistics, a comprehensive set of factors, namely, sample size, correlational structure, test length, item quality, and generating model, is systematically manipulated using Monte Carlo methods. Results show that the S-X2 statistic has unacceptable power. Type I error and power comparisons favor LR and W tests over the LM test. However, all the statistics are highly affected by the item quality. With a few exceptions, their performance is only acceptable when the item quality is high. In some cases, this effect can be ameliorated by an increase in sample size and test length. This implies that using the above statistics to assess item fit in practical settings when the item quality is low remains a challenge.},
 author = {Sorrel, Miguel A. and Abad, Francisco J. and Olea, Julio and {de la Torre}, Jimmy and Barrada, Juan Ram{\'o}n},
 year = {2017},
 title = {Inferential Item-Fit Evaluation in Cognitive Diagnosis Modeling},
 keywords = {absolute fit;cognitive diagnosis models;item-fit statistics;power;relative fit;Type I error},
 pages = {614--631},
 volume = {41},
 number = {8},
 issn = {0146-6216},
 journal = {Applied Psychological Measurement},
 doi = {10.1177/0146621617707510},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/29882533},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5978477},
 file = {0146621617707510.pdf}
}


@article{SorrelOlea.2016,
 author = {Sorrel, Miguel A. and Olea, Julio and Abad, Francisco J. and de {La Torre}, Jimmy and Aguado, David and Lievens, Filip},
 year = {2016},
 title = {Validity and Reliability of Situational Judgement Test Scores},
 pages = {506--532},
 volume = {19},
 number = {3},
 issn = {1094-4281},
 journal = {Organizational Research Methods},
 doi = {10.1177/1094428116630065},
 file = {Validity and reliability of situational judgement test scores_ A.pdf}
}


@article{Stout.2007,
 author = {Stout, William},
 year = {2007},
 title = {Skills Diagnosis Using {IRT}-Based Continuous Latent Trait Models},
 pages = {313--324},
 volume = {44},
 number = {4},
 issn = {0022-0655},
 journal = {Journal of Educational Measurement},
 doi = {10.1111/j.1745-3984.2007.00041.x},
 file = {Roussos, Templin, Henson - 2007 - Skills Diagnosis Using IRT-Based Latent Class Models.pdf}
}


@incollection{StoutHenson.2019,
 author = {Stout, William and Henson, Robert and DiBello, Lou and Shear, Benjamin},
 title = {The Reparameterized Unified Model System: A Diagnostic Assessment Modeling Approach},
 pages = {47--79},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-030-05583-7},
 editor = {{von Davier}, Matthias and Lee, Young-Sun},
 booktitle = {Handbook of Diagnostic Classification Models},
 year = {2019},
 address = {Cham},
 doi = {10.1007/978-3-030-05584-4{\textunderscore }3}
}


@article{SunXin.2013,
 author = {Sun, Jianan and Xin, Tao and Zhang, Shumei and {de la Torre}, Jimmy},
 year = {2013},
 title = {A Polytomous Extension of the Generalized Distance Discriminating Method},
 pages = {503--521},
 volume = {37},
 number = {7},
 issn = {0146-6216},
 journal = {Applied Psychological Measurement},
 doi = {10.1177/0146621613487254},
 file = {Sun et al. - 2013 - A Polytomous Extension of the Generalized Distance Discriminating Method.pdf}
}


@article{Tatsuoka.1983,
 author = {Tatsuoka, Kikumi K.},
 year = {1983},
 title = {RULE SPACE: AN APPROACH FOR DEALING WITH MISCONCEPTIONS BASED ON ITEM RESPONSE THEORY},
 pages = {345--354},
 volume = {20},
 number = {4},
 issn = {0022-0655},
 journal = {Journal of Educational Measurement},
 doi = {10.1111/j.1745-3984.1983.tb00212.x},
 file = {Tatsuoka - 1983 - Rule space An approach for dealing with misconceptions based on item response theory.pdf}
}


@book{Tatsuoka.2009,
 author = {Tatsuoka, Kikumi K.},
 year = {2009},
 title = {Cognitive assessment: An introduction to the rule space method},
 price = {{\pounds}60.00},
 keywords = {Cognition   Testing},
 address = {London},
 publisher = {{Routledge Academic}},
 isbn = {0203883373},
 series = {Multivariate applications series}
}


@article{TemplinBradshaw.2013,
 author = {Templin, Jonathan and Bradshaw, Laine},
 year = {2013},
 title = {Measuring the Reliability of Diagnostic Classification Model Examinee Estimates},
 pages = {251--275},
 volume = {30},
 number = {2},
 issn = {0176-4268},
 journal = {Journal of Classification},
 doi = {10.1007/s00357-013-9129-4},
 file = {Templin, Bradshaw - 2013 - Measuring the Reliability of Diagnostic Classification Model Examinee Estimates.pdf}
}


@article{TemplinBradshaw.2014,
 abstract = {Although latent attributes that follow a hierarchical structure are anticipated in many areas of educational and psychological assessment, current psychometric models are limited in their capacity to objectively evaluate the presence of such attribute hierarchies. This paper introduces the Hierarchical Diagnostic Classification Model (HDCM), which adapts the Log-linear Cognitive Diagnosis Model to cases where attribute hierarchies are present. The utility of the HDCM is demonstrated through simulation and by an empirical example. Simulation study results show the HDCM is efficiently estimated and can accurately test for the presence of an attribute hierarchy statistically, a feature not possible when using more commonly used DCMs. Empirically, the HDCM is used to test for the presence of a suspected attribute hierarchy in a test of English grammar, confirming the data is more adequately represented by hierarchical attribute structure when compared to a crossed, or nonhierarchical structure.},
 author = {Templin, Jonathan and Bradshaw, Laine},
 year = {2014},
 title = {Hierarchical diagnostic classification models: a family of models for estimating and testing attribute hierarchies},
 keywords = {Classification/methods;Educational Measurement/methods;Humans;Psychometrics/methods;Statistics as Topic/methods},
 pages = {317--339},
 volume = {79},
 number = {2},
 journal = {Psychometrika},
 doi = {10.1007/s11336-013-9362-0},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/24478021},
 file = {Templin-Bradshaw2014_Article_HierarchicalDiagnosticClassifi.pdf}
}


@article{TemplinHenson.2006,
 abstract = {Cognitive diagnosis models are constrained (multiple classification) latent class models that characterize the relationship of questionnaire responses to a set of dichotomous latent variables. Having emanated from educational measurement, several aspects of such models seem well suited to use in psychological assessment and diagnosis. This article presents the development of a new cognitive diagnosis model for use in psychological assessment--the DINO (deterministic input; noisy {\textquotedbl}or{\textquotedbl} gate) model--which, as an illustrative example, is applied to evaluate and diagnose pathological gamblers. As part of this example, a demonstration of the estimates obtained by cognitive diagnosis models is provided. Such estimates include the probability an individual meets each of a set of dichotomous Diagnostic and Statistical Manual of Mental Disorders (text revision [DSM-IV-TR]; American Psychiatric Association, 2000) criteria, resulting in an estimate of the probability an individual meets the DSM-IV-TR definition for being a pathological gambler. Furthermore, a demonstration of how the hypothesized underlying factors contributing to pathological gambling can be measured with the DINO model is presented, through use of a covariance structure model for the tetrachoric correlation matrix of the dichotomous latent variables representing DSM-IV-TR criteria.},
 author = {Templin, Jonathan L. and Henson, Robert A.},
 year = {2006},
 title = {Measurement of psychological disorders using cognitive diagnosis models},
 keywords = {Cognition Disorders/diagnosis;Data Interpretation, Statistical;Gambling;Humans;Mental Disorders/diagnosis;Models, Psychological;Psychology/methods/statistics {\&} numerical data},
 pages = {287--305},
 volume = {11},
 number = {3},
 journal = {Psychological methods},
 doi = {10.1037/1082-989X.11.3.287},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/16953706},
 file = {Templin, Henson - 2006 - Measurement of psychological disorders using cognitive diagnosis models.pdf}
}


@article{TjoedelaTorre.2014,
 author = {Tjoe, Hartono and {de la Torre}, Jimmy},
 year = {2014},
 title = {The identification and validation process of proportional reasoning attributes: an application of a cognitive diagnosis modeling framework},
 pages = {237--255},
 volume = {26},
 number = {2},
 issn = {1033-2170},
 journal = {Mathematics Education Research Journal},
 doi = {10.1007/s13394-013-0090-7},
 file = {Tjoe, de la Torre - 2014 - The identification and validation process of proportional reasoning attributes An application of a cognitive.pdf}
}


@article{TuGao.2017,
 abstract = {To obtain accurate, valid, and rich information from the questionnaires for internet addiction, a diagnostic classification test for internet addiction (the DCT-IA) was developed using diagnostic classification models (DCMs), a cutting-edge psychometric theory, based on DSM-5. A calibration sample and a validation sample were recruited in this study to calibrate the item parameters of the DCT-IA and to examine the sensitivity and specificity. The DCT-IA had high reliability and validity based on both CTT and DCMs, and had a sensitivity of 0.935 and a specificity of 0.817 with AUC = 0.919. More important, different from traditional questionnaires, the DCT-IA can simultaneously provide general-level diagnostic information and the detailed symptom criteria-level information about the posterior probability of satisfying each symptom criterion in DMS-5 for each patient, which gives insight into tailoring individual-specific treatments for internet addiction.},
 author = {Tu, Dongbo and Gao, Xuliang and Wang, Daxun and CAI, Yan},
 year = {2017},
 title = {A New Measurement of Internet Addiction Using Diagnostic Classification Models},
 keywords = {cognitive diagnosis models;Diagnostic classification models;internet addiction;measurement;symptom criteria-level information},
 pages = {1768},
 volume = {8},
 issn = {1664-1078},
 journal = {Frontiers in psychology},
 doi = {10.3389/fpsyg.2017.01768},
 file = {Tu et al. - 2017 - A New Measurement of Internet Addiction Using Diagnostic Classification Models.pdf},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/29066994},
 file = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5641364}
}


@article{TuZheng.2018,
 author = {Tu, Dongbo and Zheng, Chanjin and CAI, Yan and Gao, Xuliang and Wang, Daxun},
 year = {2018},
 title = {A Polytomous Model of Cognitive Diagnostic Assessment for Graded Data},
 pages = {231--252},
 volume = {18},
 number = {3},
 issn = {1530-5058},
 journal = {International Journal of Testing},
 doi = {10.1080/15305058.2017.1396465},
 file = {6.International of Testing A Polytomous Model of CDMs.pdf}
}


@article{vonDavier.2008,
 abstract = {Probabilistic models with one or more latent variables are designed to report on a corresponding number of skills or cognitive attributes. Multidimensional skill profiles offer additional information beyond what a single test score can provide, if the reported skills can be identified and distinguished reliably. Many recent approaches to skill profile models are limited to dichotomous data and have made use of computationally intensive estimation methods such as Markov chain Monte Carlo, since standard maximum likelihood (ML) estimation techniques were deemed infeasible. This paper presents a general diagnostic model (GDM) that can be estimated with standard ML techniques and applies to polytomous response variables as well as to skills with two or more proficiency levels. The paper uses one member of a larger class of diagnostic models, a compensatory diagnostic model for dichotomous and partial credit data. Many well-known models, such as univariate and multivariate versions of the Rasch model and the two-parameter logistic item response theory model, the generalized partial credit model, as well as a variety of skill profile models, are special cases of this GDM. In addition to an introduction to this model, the paper presents a parameter recovery study using simulated data and an application to real data from the field test for TOEFL Internet-based testing.},
 author = {{von Davier}, Matthias},
 year = {2008},
 title = {A general diagnostic model applied to language testing data},
 keywords = {Humans;Language Tests;Linguistics/statistics {\&} numerical data;Models, Psychological;Reaction Time},
 pages = {287--307},
 volume = {61},
 number = {Pt 2},
 issn = {0007-1102},
 journal = {The British journal of mathematical and statistical psychology},
 doi = {10.1348/000711007X193957},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/17535481},
 file = {von Davier - 2008 - A general diagnostic model applied to language testing data.pdf}
}


@article{vonDavier.2010,
 author = {{von Davier}, Matthias},
 year = {2010},
 title = {Hierarchical mixtures of diagnostic models},
 urldate = {2/7/2020},
 volume = {52},
 journal = {Psychological Test and Assessment Modeling},
 file = {Vahrenhorst - Microsoft Word.pdf}
}


@book{vonDavierLee.2019,
 year = {2019},
 title = {Handbook of Diagnostic Classification Models},
 address = {Cham},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-030-05583-7},
 editor = {{von Davier}, Matthias and Lee, Young-Sun},
 doi = {10.1007/978-3-030-05584-4},
 file = {2019_Book_HandbookOfDiagnosticClassifica.pdf}
}


@article{WangChangDouglas.2012,
 abstract = {Computerized adaptive testing (CAT) was originally proposed to measure \textgreek{j}, usually a latent trait, with greater precision by sequentially selecting items according to the student's responses to previously administered items. Although the application of CAT is promising for many educational testing programs, most of the current CAT systems were not designed to provide diagnostic information. This article discusses item selection strategies specifically tailored for cognitive diagnostic tests. Our goal is to identify an effective item selection algorithm that not only estimates \textgreek{j} efficiently, but also classifies the student's knowledge status \textgreek{a} accurately. A single-stage item selection method with a dual purpose will be introduced. The main idea is to treat diagnostic criteria as constraints: Using the maximum priority index method to meet these constraints, the CAT system is able to generate cognitive diagnostic feedback in a fairly straightforward fashion. Different priority functions are proposed. Some of them are based on certain information measures, such as Kullback-Leibler information, and others utilize only the information provided by the Q-matrix. An extensive simulation study is conducted, and the results indicate that the information-based method not only yields higher classification rates for cognitive diagnosis, but also achieves more accurate \textgreek{j} estimation. Other constraint controls, such as item exposure rates, are also considered for all the competing methods.},
 author = {Wang, Chun and Chang, Hua-Hua and Douglas, Jeffery},
 year = {2012},
 title = {Combining {CAT} with cognitive diagnosis: a weighted item selection approach},
 keywords = {Computer Simulation;Educational Measurement/methods;Humans;Models, Statistical;Software},
 pages = {95--109},
 volume = {44},
 number = {1},
 journal = {Behavior research methods},
 doi = {10.3758/s13428-011-0143-3},
 file = {11e1fc98-b9d6-404a-b1bc-a13b76e5b1b8:C\:\\Users\\wench\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\hzsl0hndqon2ipk638qbntgdllf1r1q5dxzhi26xkkmxy8hcs6\\Citavi Attachments\\11e1fc98-b9d6-404a-b1bc-a13b76e5b1b8.pdf:pdf}
}


@article{WangShu.2015,
 abstract = {This research focuses on developing item-level fit checking procedures in the context of diagnostic classification models (DCMs), and more specifically for the {\textquotedbl}Deterministic Input; Noisy 'And' gate{\textquotedbl} (DINA) model. Although there is a growing body of literature discussing model fit checking methods for DCM, the item-level fit analysis is not adequately discussed in literature. This study intends to take an initiative to fill in this gap. Two approaches are proposed, one stems from classical goodness-of-fit test statistics coupled with the Expectation-Maximization algorithm for model estimation, and the other is the posterior predictive model checking (PPMC) method coupled with the Markov chain Monte Carlo estimation. For both approaches, the chi-square statistic and a power-divergence index are considered, along with Stone's method for considering uncertainty in latent attribute estimation. A simulation study with varying manipulated factors is carried out. Results show that both approaches are promising if Stone's method is imposed, but the classical goodness-of-fit approach has a much higher detection rate (i.e., proportion of misfit items that are correctly detected) than the PPMC method.},
 author = {Wang, Chun and Shu, Zhan and Shang, Zhuoran and Xu, Gongjun},
 year = {2015},
 title = {Assessing Item-Level Fit for the {DINA} Model},
 pages = {525--538},
 volume = {39},
 number = {7},
 issn = {0146-6216},
 journal = {Applied Psychological Measurement},
 doi = {10.1177/0146621615583050},
 file = {550be2f4-4c08-429e-b4ef-73c6e8a626ab:C\:\\Users\\wench\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\hzsl0hndqon2ipk638qbntgdllf1r1q5dxzhi26xkkmxy8hcs6\\Citavi Attachments\\550be2f4-4c08-429e-b4ef-73c6e8a626ab.pdf:pdf}
}


@article{WangSong.2015,
 author = {Wang, Wenyi and Song, Lihong and Chen, Ping and Meng, Yaru and DING, Shuliang},
 year = {2015},
 title = {Attribute--Level and Pattern--Level Classification Consistency and Accuracy Indices for Cognitive Diagnostic Assessment},
 pages = {457--476},
 volume = {52},
 number = {4},
 journal = {Journal of Educational Measurement},
 doi = {10.1111/jedm.12096},
 file = {Wang, Song et al 2015 - Attribute-Level and Pattern-Level Classification Consistency.pdf},
 file = {https://onlinelibrary.wiley.com/doi/full/10.1111/jedm.12096},
 file = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jedm.12096}
}


@article{WangYang.2018,
 abstract = {Journal of Educational and Behavioral Statistics 2018.43:57-87},
 author = {Wang, Shiyu and Yang, Yan and Culpepper, Steven Andrew and Douglas, Jeffrey A.},
 year = {2018},
 title = {Tracking Skill Acquisition With Cognitive Diagnosis Models: A Higher-Order, Hidden Markov Model With Covariates},
 keywords = {cognitive diagnostic models;cognitive diagnostic models,higher order,hidden Markov model,longitudinal,skill change,Markov chain Monte Carlo,spatial cognition;hidden Markov model;higher order;longitudinal;Markov chain Monte Carlo;skill change;spatial cognition},
 urldate = {2/7/2020},
 pages = {57--87},
 volume = {43},
 number = {1},
 issn = {1076-9986},
 journal = {Journal of Educational and Behavioral Statistics},
 doi = {10.3102/1076998617719727},
 file = {Wang, Yang et al. 2018 - Tracking Skill Acquisition With Cognitive.pdf}
}


@article{WangZhengChang.2014,
 abstract = {Journal of Educational Measurement 2014.51:358-380},
 author = {Wang, Chun and Zheng, Chanjin and Chang, Hua-Hua},
 year = {2014},
 title = {An Enhanced Approach to Combine Item Response Theory With Cognitive Diagnosis in Adaptive Testing},
 urldate = {2/23/2020},
 pages = {358--380},
 volume = {51},
 number = {4},
 issn = {0022-0655},
 journal = {Journal of Educational Measurement},
 doi = {10.1111/jedm.12057},
 file = {3c4c2a43-a6e3-406e-bd86-bcb468b59216:C\:\\Users\\wench\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\hzsl0hndqon2ipk638qbntgdllf1r1q5dxzhi26xkkmxy8hcs6\\Citavi Attachments\\3c4c2a43-a6e3-406e-bd86-bcb468b59216.pdf:pdf}
}


@article{Xu.2017,
 abstract = {The Annals of Statistics, 2017, Vol. 45, No. 2, 675-707},
 author = {Xu, Gongjun},
 year = {2017},
 title = {Identifiability of restricted latent class models with binary responses},
 keywords = {62E10;62E10, Identifiability, restricted latent class models, Q-matrix, cognitive diagnosis models, multivariate Bernoulli mixture, Kruskal's tensor decomposition;cognitive diagnosis models;Identifiability;Kruskal's tensor decomposition;multivariate Bernoulli mixture;Q-matrix;Restricted latent class models},
 urldate = {2/19/2020},
 pages = {675--707},
 volume = {45},
 number = {2},
 issn = {0090-5364},
 journal = {The Annals of Statistics},
 doi = {10.1214/16-AOS1464},
 file = {Xu 2017 - Identifiability of restricted latent class.pdf}
}


@article{XuShang.2018,
 abstract = {Journal of the American Statistical Association, 2018. doi:10.1080/01621459.2017.1340889},
 author = {Xu, Gongjun and Shang, Zhuoran},
 year = {2018},
 title = {Identifying Latent Structures in Restricted Latent Class Models},
 keywords = {cognitive diagnosis;Identifiability;Q-matrix;Restricted latent class models},
 pages = {1284--1295},
 volume = {113},
 number = {523},
 issn = {0162-1459},
 journal = {Journal of the American Statistical Association},
 doi = {10.1080/01621459.2017.1340889},
 file = {Identifying Latent Structures in Restricted Latent Class Models.pdf}
}


@article{XuWangShang.2016,
 abstract = {There has recently been much interest in computerized adaptive testing (CAT) for cognitive diagnosis. While there exist various item selection criteria and different asymptotically optimal designs, these are mostly constructed based on the asymptotic theory assuming the test length goes to infinity. In practice, with limited test lengths, the desired asymptotic optimality may not always apply, and there are few studies in the literature concerning the optimal design of finite items. Related questions, such as how many items we need in order to be able to identify the attribute pattern of an examinee and what types of initial items provide the optimal classification results, are still open. This paper aims to answer these questions by providing non-asymptotic theory of the optimal selection of initial items in cognitive diagnostic CAT. In particular, for the optimal design, we provide necessary and sufficient conditions for the Q-matrix structure of the initial items. The theoretical development is suitable for a general family of cognitive diagnostic models. The results not only provide a guideline for the design of optimal item selection procedures, but also may be applied to guide item bank construction.},
 author = {Xu, Gongjun and Wang, Chun and Shang, Zhuoran},
 year = {2016},
 title = {On initial item selection in cognitive diagnostic computerized adaptive testing},
 keywords = {Algorithms;Cognition Disorders/diagnosis;Computer Simulation;Data Interpretation, Statistical;Diagnosis, Computer-Assisted/methods;Educational Measurement/methods;Humans;Models, Statistical;Psychometrics/methods;Reproducibility of Results;Sensitivity and Specificity;Surveys and Questionnaires},
 urldate = {2/23/2020},
 pages = {291--315},
 volume = {69},
 number = {3},
 issn = {0007-1102},
 journal = {The British journal of mathematical and statistical psychology},
 doi = {10.1111/bmsp.12072},
 file = {7fed65b3-4e01-4600-a16d-cf24ccaeb097:C\:\\Users\\wench\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\hzsl0hndqon2ipk638qbntgdllf1r1q5dxzhi26xkkmxy8hcs6\\Citavi Attachments\\7fed65b3-4e01-4600-a16d-cf24ccaeb097.pdf:pdf}
}


@article{XuZhang.2016,
 abstract = {Diagnostic classification models (DCMs) are important statistical tools in cognitive diagnosis. In this paper, we consider the issue of their identifiability. In particular, we focus on one basic and popular model, the DINA model. We propose sufficient and necessary conditions under which the model parameters are identifiable from the data. The consequences, in terms of the consistency of parameter estimates, of fulfilling or failing to fulfill these conditions are illustrated via simulation. The results can be easily extended to the DINO model through the duality of the DINA and DINO models. Moreover, the proposed theoretical framework could be applied to study the identifiability issue of other DCMs.},
 author = {Xu, Gongjun and Zhang, Stephanie},
 year = {2016},
 title = {Identifiability of Diagnostic Classification Models},
 keywords = {diagnostic classification models the DINA model model identifiability Q-matrix;Educational Measurement;Humans;Models, Statistical},
 pages = {625--649},
 volume = {81},
 number = {3},
 journal = {Psychometrika},
 doi = {10.1007/s11336-015-9471-z},
 file = {http://www.ncbi.nlm.nih.gov/pubmed/26155755},
 file = {Xu, Zhang - 2016 - Identifiability of Diagnostic Classification Models.pdf}
}


@article{Yamamoto.1982,
 author = {Yamamoto, Kentaro},
 year = {1982},
 title = {HYBRID MODEL OF {IRT} AND LATENT CLASS MODELS},
 pages = {i-61},
 volume = {1982},
 number = {2},
 issn = {23308516},
 journal = {ETS Research Report Series},
 doi = {10.1002/j.2333-8504.1982.tb01326.x},
 file = {Yamamoto - 1989 - Hybrid Model of IRT and Latent Class Models.pdf}
}


@article{YamamotoEverson.1995,
 author = {Yamamoto, Kentaro and Everson, Howard T.},
 year = {1995},
 title = {MODELING THE MIXTURE OF {IRT} AND PATTERN RESPONSES BY A MODIFIED HYBRID MODEL1},
 pages = {i-26},
 volume = {1995},
 number = {1},
 issn = {23308516},
 journal = {ETS Research Report Series},
 doi = {10.1002/j.2333-8504.1995.tb01651.x},
 file = {Yamamoto, Everson - 1995 - Modeling the mixture of irt and pattern responses by a modified hybrid model.pdf}
}


@article{YigitSorrelLaTorre.2019,
 abstract = {Cognitive diagnosis models (CDMs) are latent class models that hold great promise for providing diagnostic information about student knowledge profiles. The increasing use of computers in classrooms enhances the advantages of CDMs for more efficient diagnostic testing by using adaptive algorithms, referred to as cognitive diagnosis computerized adaptive testing (CD-CAT). When multiple-choice items are involved, CD-CAT can be further improved by using polytomous scoring (i.e., considering the specific options students choose), instead of dichotomous scoring (i.e., marking answers as either right or wrong). In this study, the authors propose and evaluate the performance of the Jensen-Shannon divergence (JSD) index as an item selection method for the multiple-choice deterministic inputs, noisy {\textquotedbl}and{\textquotedbl} gate (MC-DINA) model. Attribute classification accuracy and item usage are evaluated under different conditions of item quality and test termination rule. The proposed approach is compared with the random selection method and an approximate approach based on dichotomized responses. The results show that under the MC-DINA model, JSD improves the attribute classification accuracy significantly by considering the information from distractors, even with a very short test length. This result has important implications in practical classroom settings as it can allow for dramatically reduced testing times, thus resulting in more targeted learning opportunities.},
 author = {Yigit, Hulya D. and Sorrel, Miguel A. and de {La Torre}, Jimmy},
 year = {2019},
 title = {Computerized Adaptive Testing for Cognitively Based Multiple-Choice Data},
 keywords = {cognitive diagnosis models;cognitive diagnosis models,computerized adaptive testing,MC-DINA,G-DINA,item selection methods,JSD,GDI;computerized adaptive testing;GDI;G-DINA;item selection methods;JSD;MC-DINA},
 urldate = {2/23/2020},
 pages = {388--401},
 volume = {43},
 number = {5},
 issn = {0146-6216},
 journal = {Applied Psychological Measurement},
 doi = {10.1177/0146621618798665},
 file = {38262e0b-7814-44b7-8eee-ea9c7295977a:C\:\\Users\\wench\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\hzsl0hndqon2ipk638qbntgdllf1r1q5dxzhi26xkkmxy8hcs6\\Citavi Attachments\\38262e0b-7814-44b7-8eee-ea9c7295977a.pdf:pdf}
}


@article{ZhanJiao.2019,
 abstract = {Journal of Educational and Behavioral Statistics 0.0:1076998619826040},
 author = {Zhan, Peida and Jiao, Hong and Man, Kaiwen and Wang, Lijun},
 year = {2019},
 title = {Using {JAGS} for Bayesian Cognitive Diagnosis Modeling: A Tutorial},
 keywords = {Bayesian estimation;cognitive diagnosis modeling;DINA model;DINO model;longitudinal diagnosis;Markov chain Monte Carlo;polytomous attributes;rRUM;testlet},
 pages = {473--503},
 volume = {44},
 number = {4},
 issn = {1076-9986},
 journal = {Journal of Educational and Behavioral Statistics},
 doi = {10.3102/1076998619826040},
 file = {1076998619826040(1).pdf}
}


@article{ZhanJiao.2019b,
 abstract = {Journal of Educational and Behavioral Statistics 2019.44:251-281},
 author = {Zhan, Peida and Jiao, Hong and Liao, Dandan and Li, Feiming},
 year = {2019},
 title = {A Longitudinal Higher-Order Diagnostic Classification Model},
 keywords = {anchor-item;cognitive diagnosis;cognitive diagnosis,diagnostic classification model,longitudinal data,anchor-item,local item dependence,DINA;diagnostic classification model;DINA;local item dependence;longitudinal data},
 urldate = {2/7/2020},
 pages = {251--281},
 volume = {44},
 number = {3},
 issn = {1076-9986},
 journal = {Journal of Educational and Behavioral Statistics},
 doi = {10.3102/1076998619827593},
 file = {Zhan, Jiao et al. 2019 - A Longitudinal Higher-Order Diagnostic Classification.pdf}
}


@article{ZhanMa.2020,
 abstract = {The higher-order structure and attribute hierarchical structure are two popular approaches to defining the latent attribute space in cognitive diagnosis models. However, to our knowledge, it is still impossible to integrate them to accommodate the higher-order latent trait and hierarchical attributes simultaneously. To address this issue, this article proposed a sequential higher-order latent structural model (LSM) by incorporating various hierarchical structures into a higher-order latent structure. The feasibility of the proposed higher-order LSM was examined using simulated data. Results indicated that, in conjunction with the deterministic-inputs, noisy {\textquotedbl}and{\textquotedbl} gate model, the sequential higher-order LSM produced considerable improvement in person classification accuracy compared with the conventional higher-order LSM, when a certain attribute hierarchy existed. An empirical example was presented as well to illustrate the application of the proposed LSM.},
 author = {Zhan, Peida and Ma, Wenchao and Jiao, Hong and DING, Shuliang},
 year = {2020},
 title = {A Sequential Higher Order Latent Structural Model for Hierarchical Attributes in Cognitive Diagnostic Assessments},
 keywords = {attribute hierarchy;cognitive diagnosis;cognitive diagnosis models;cognitive diagnosis,higher-order latent structure,attribute hierarchy,sequential tree,cognitive diagnosis models,DINA model;DINA model;higher-order latent structure;sequential tree},
 urldate = {2/23/2020},
 pages = {65--83},
 volume = {44},
 number = {1},
 issn = {0146-6216},
 journal = {Applied Psychological Measurement},
 doi = {10.1177/0146621619832935},
 file = {3cf3d59f-3aa6-4987-9102-60cfc9f776d5:C\:\\Users\\wench\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\hzsl0hndqon2ipk638qbntgdllf1r1q5dxzhi26xkkmxy8hcs6\\Citavi Attachments\\3cf3d59f-3aa6-4987-9102-60cfc9f776d5.pdf:pdf}
}


@article{ZhanWangLi.2019,
 author = {Zhan, Peida and Wang, Wen-Chung and Li, Xiaomin},
 year = {2019},
 title = {A Partial Mastery, Higher-Order Latent Structural Model for Polytomous Attributes in Cognitive Diagnostic Assessments},
 pages = {195},
 volume = {30},
 number = {2},
 issn = {0176-4268},
 journal = {Journal of Classification},
 doi = {10.1007/s00357-019-09323-7},
 file = {priprint_researchgateAPartialMasteryHigher-OrderLSM.pdf}
}


@article{ZhengWang.2017,
 abstract = {Cognitive diagnosis has emerged as a new generation of testing theory for educational assessment after the item response theory (IRT). One distinct feature of cognitive diagnostic models (CDMs) is that they assume the latent trait to be discrete instead of continuous as in IRT. From this perspective, cognitive diagnosis bears a close resemblance to searching problems in computer science and, similarly, item selection problem in cognitive diagnostic computerized adaptive testing (CD-CAT) can be considered as a dynamic searching problem. Previously, item selection algorithms in CD-CAT were developed from information indices in information science and attempted to achieve a balance among several objectives by assigning different weights. As a result, they suffered from low efficiency from a tug-of-war competition among multiple goals in item selection and, at the same time, put an undue responsibility of assigning the weights for these goals by trial and error on users. Based on the searching problem perspective on CD-CAT, this article adapts the binary searching algorithm, one of the most well-known searching algorithms in searching problems, to item selection in CD-CAT. The two new methods, the stratified dynamic binary searching (SDBS) algorithm for fixed-length CD-CAT and the dynamic binary searching (DBS) algorithm for variable-length CD-CAT, can achieve multiple goals without any of the aforementioned issues. The simulation studies indicate their performances are comparable or superior to the previous methods.},
 author = {Zheng, Chanjin and Wang, Chun},
 year = {2017},
 title = {Application of Binary Searching for Item Exposure Control in Cognitive Diagnostic Computerized Adaptive Testing},
 keywords = {binary searching;CD-CAT;CD-CAT,searching algorithms,binary searching,restrictive progressive (RP) method,restrictive threshold (RT) method,SHTVOR;restrictive progressive (RP) method;restrictive threshold (RT) method;searching algorithms;SHTVOR},
 urldate = {2/23/2020},
 pages = {561--576},
 volume = {41},
 number = {7},
 issn = {0146-6216},
 journal = {Applied Psychological Measurement},
 doi = {10.1177/0146621617707509},
 file = {e0afd3c8-55ae-4aa1-88c1-52377c347318:C\:\\Users\\wench\\AppData\\Local\\Swiss Academic Software\\Citavi 6\\ProjectCache\\hzsl0hndqon2ipk638qbntgdllf1r1q5dxzhi26xkkmxy8hcs6\\Citavi Attachments\\e0afd3c8-55ae-4aa1-88c1-52377c347318.pdf:pdf}
}


